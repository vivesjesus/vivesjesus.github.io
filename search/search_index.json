{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"INTRODUCCI\u00d3N \u00b6 \"Los datos son el nuevo petr\u00f3leo\" - Clive Humby, 2006 \u00bfD\u00f3nde nacen los datos de hoy en d\u00eda? \u00b6 Los datos que nosotros utilizamos hoy en d\u00eda se generan principalmente a trav\u00e9s de nuestro m\u00f3vil, la sens\u00f3rica y los sistemas de gesti\u00f3n de n\u00f3minas, contabilidad, etc. Note Cantidad de datos que se generan por minuto en el a\u00f1o 2020 Definiciones \u00b6 DATA SCIENCE - CIENCIA DE DATOS - Es una ciencia interdisciplinar basada en el an\u00e1lisis de datos. Su objetivo es generar valor a partir de la recopilaci\u00f3n, clasificaci\u00f3n, visualizaci\u00f3n e interpretaci\u00f3n de datos. BIG DATA - hace referencia a un volumen de datos tan \u00e1mplio y complejo de datos que las t\u00e9cnicas tradicionales no pueden abordar siendo necesaria una aproximaci\u00f3n espec\u00edfica para este tipo de problemas. BUSINESS INTELLIGENCE - abarca todas las tecnolog\u00edas, aplicaciones y pr\u00e1cticas para recolectar, integrar y analizar la informaci\u00f3n mediante un an\u00e1lisis descriptivo par mejorar la toma de decisiones. BUSINESS ANALYTICS - recolecta, integra y analiza datos pero var\u00eda el enfoque. Introduce modelos predictivos para establecer tendencias, averiguar por qu\u00e9 suceden las cosas y hacer una estimaci\u00f3n de c\u00f3mo se desarrollar\u00e1n las cosas en el futuro. INTELIGENCIA ARTIFICIAL - son los sistemas o m\u00e1quinas que imitan la inteligencia humana para realizar tareas y pueden mejorar iterativamente a partir de la informaci\u00f3n que recopilan. Data Warehouse y Data Lake \u00b6 \u00bfD\u00f3nde almacenamos toda la informaci\u00f3n que tenemos? Data Warehouse - es el lugar donde a trav\u00e9s de una base de datos se integran todos los datos que tengo para despu\u00e9s poder visualizar esos datos, aplicar algoritmos, etc. Es decir, es la centralizaci\u00f3n de todos los datos en una misma base de datos. Es una base de datos estructurada. Data Lake - Es similar al data Warehouse pero la informaci\u00f3n no est\u00e1 estructurada. Se guarda el dato en crudo (json, im\u00e1genes, v\u00eddeos, csv,...). Es un tipo de repositorio que almacena conjuntos grandes y diversos de datos sin procesar en su formato original y que mantiene una perspectiva general de \u00e9stos. Beneficios y usos del Big Data \u00b6 Los beneficios del big data son: Mejora de la toma de decisiones. Mejora en la eficiencia y optimizaci\u00f3n de costes. Segmentaci\u00f3n de los clientes. Seguridad en los datos. Mejora de la accesibilidad de la informaci\u00f3n dentro de la empresa. Ventajas competitivas. La ciencia de datos y el Big Data se utilizan tanto en entornos comerciales como en no comerciales: Las empresas comerciales lo utilizan para obtener informaci\u00f3n sobre sus clientes, procesos, personal, productos, etc. Las instituciones financieras utilizan la ciencia de datos para predecir los mercados de valores, determinar el riesgo de prestar dinero y aprender a atraer nuevos clientes para sus servicios. Las organizaciones gubernamentales tambi\u00e9n son conscientes del valor de los datos. Muchas organizaciones no solo dependen de los cient\u00edficos de datos internos para descubrir informaci\u00f3n valiosa sino que tambi\u00e9n comparten sus datos con el p\u00fablico. Pueden utlizar estos datos para obtener conocimientos o crear aplicaciones basadas en datos. Las organizaciones no gubernamentales (ONG) tampoco son ajenas al uso de datos. Lo utilizan para recaudar fondos y defender sus causas. El Fondo Mundial para la Naturaleza (WWF) , por ejemplo, emplea cient\u00edficos de datos para aumentar su efectividad de sus esfuerzos de recaudaci\u00f3n de fondos. Las universidades utilizan la ciencia de datos en sus investigaciones, pero tambi\u00e9n para mejorar la experiencia de estudio de sus estudiantes.","title":"1. INTRODUCCI\u00d3N"},{"location":"index.html#introduccion","text":"\"Los datos son el nuevo petr\u00f3leo\" - Clive Humby, 2006","title":"INTRODUCCI\u00d3N"},{"location":"index.html#donde-nacen-los-datos-de-hoy-en-dia","text":"Los datos que nosotros utilizamos hoy en d\u00eda se generan principalmente a trav\u00e9s de nuestro m\u00f3vil, la sens\u00f3rica y los sistemas de gesti\u00f3n de n\u00f3minas, contabilidad, etc. Note Cantidad de datos que se generan por minuto en el a\u00f1o 2020","title":"\u00bfD\u00f3nde nacen los datos de hoy en d\u00eda?"},{"location":"index.html#definiciones","text":"DATA SCIENCE - CIENCIA DE DATOS - Es una ciencia interdisciplinar basada en el an\u00e1lisis de datos. Su objetivo es generar valor a partir de la recopilaci\u00f3n, clasificaci\u00f3n, visualizaci\u00f3n e interpretaci\u00f3n de datos. BIG DATA - hace referencia a un volumen de datos tan \u00e1mplio y complejo de datos que las t\u00e9cnicas tradicionales no pueden abordar siendo necesaria una aproximaci\u00f3n espec\u00edfica para este tipo de problemas. BUSINESS INTELLIGENCE - abarca todas las tecnolog\u00edas, aplicaciones y pr\u00e1cticas para recolectar, integrar y analizar la informaci\u00f3n mediante un an\u00e1lisis descriptivo par mejorar la toma de decisiones. BUSINESS ANALYTICS - recolecta, integra y analiza datos pero var\u00eda el enfoque. Introduce modelos predictivos para establecer tendencias, averiguar por qu\u00e9 suceden las cosas y hacer una estimaci\u00f3n de c\u00f3mo se desarrollar\u00e1n las cosas en el futuro. INTELIGENCIA ARTIFICIAL - son los sistemas o m\u00e1quinas que imitan la inteligencia humana para realizar tareas y pueden mejorar iterativamente a partir de la informaci\u00f3n que recopilan.","title":"Definiciones"},{"location":"index.html#data-warehouse-y-data-lake","text":"\u00bfD\u00f3nde almacenamos toda la informaci\u00f3n que tenemos? Data Warehouse - es el lugar donde a trav\u00e9s de una base de datos se integran todos los datos que tengo para despu\u00e9s poder visualizar esos datos, aplicar algoritmos, etc. Es decir, es la centralizaci\u00f3n de todos los datos en una misma base de datos. Es una base de datos estructurada. Data Lake - Es similar al data Warehouse pero la informaci\u00f3n no est\u00e1 estructurada. Se guarda el dato en crudo (json, im\u00e1genes, v\u00eddeos, csv,...). Es un tipo de repositorio que almacena conjuntos grandes y diversos de datos sin procesar en su formato original y que mantiene una perspectiva general de \u00e9stos.","title":"Data Warehouse y Data Lake"},{"location":"index.html#beneficios-y-usos-del-big-data","text":"Los beneficios del big data son: Mejora de la toma de decisiones. Mejora en la eficiencia y optimizaci\u00f3n de costes. Segmentaci\u00f3n de los clientes. Seguridad en los datos. Mejora de la accesibilidad de la informaci\u00f3n dentro de la empresa. Ventajas competitivas. La ciencia de datos y el Big Data se utilizan tanto en entornos comerciales como en no comerciales: Las empresas comerciales lo utilizan para obtener informaci\u00f3n sobre sus clientes, procesos, personal, productos, etc. Las instituciones financieras utilizan la ciencia de datos para predecir los mercados de valores, determinar el riesgo de prestar dinero y aprender a atraer nuevos clientes para sus servicios. Las organizaciones gubernamentales tambi\u00e9n son conscientes del valor de los datos. Muchas organizaciones no solo dependen de los cient\u00edficos de datos internos para descubrir informaci\u00f3n valiosa sino que tambi\u00e9n comparten sus datos con el p\u00fablico. Pueden utlizar estos datos para obtener conocimientos o crear aplicaciones basadas en datos. Las organizaciones no gubernamentales (ONG) tampoco son ajenas al uso de datos. Lo utilizan para recaudar fondos y defender sus causas. El Fondo Mundial para la Naturaleza (WWF) , por ejemplo, emplea cient\u00edficos de datos para aumentar su efectividad de sus esfuerzos de recaudaci\u00f3n de fondos. Las universidades utilizan la ciencia de datos en sus investigaciones, pero tambi\u00e9n para mejorar la experiencia de estudio de sus estudiantes.","title":"Beneficios y usos del Big Data"},{"location":"apuntes009.html","text":"Google Colaboratory \u00b6 Tambi\u00e9n llamado Colab. Permite escribir y ejecutar c\u00f3digo Python en el navegador. Es un Jupyter notebook almacenado en Google Drive. Uso gratuito de GPUs y TPUs. Los notebooks se almacenan en Google Drive, pero tambi\u00e9n se pueden cargar desde GitHub. Se puede compartir como cualquier archivo de Drive. El c\u00f3digo se ejecuta en una m\u00e1quina virtual exclusiva para tu cuenta. Existen unos l\u00edmites de uso para los recursos. Las m\u00e1quinas virtuales se borran cuando est\u00e1n inactivas durante un tiempo prolongado.","title":"4.1. GOOGLE COLABORATORY"},{"location":"apuntes009.html#google-colaboratory","text":"Tambi\u00e9n llamado Colab. Permite escribir y ejecutar c\u00f3digo Python en el navegador. Es un Jupyter notebook almacenado en Google Drive. Uso gratuito de GPUs y TPUs. Los notebooks se almacenan en Google Drive, pero tambi\u00e9n se pueden cargar desde GitHub. Se puede compartir como cualquier archivo de Drive. El c\u00f3digo se ejecuta en una m\u00e1quina virtual exclusiva para tu cuenta. Existen unos l\u00edmites de uso para los recursos. Las m\u00e1quinas virtuales se borran cuando est\u00e1n inactivas durante un tiempo prolongado.","title":"Google Colaboratory"},{"location":"apuntes01.html","text":"Big Data \u00b6 Sabias que ... \"Cada dos d\u00edas creamos tanta informaci\u00f3n como la que se cre\u00f3 desde el amanecer de la civilizaci\u00f3n hasta 2003\" Erick Schimidt- CEO Google Introducci\u00f3n \u00b6 El gran volumen de datos que ha comenzado a generarse en la \u00faltima d\u00e9cada debido a la capacidad de almacenamiento y procesamiento surgida por nuevas tecnolog\u00edas. La informaci\u00f3n referente a una empresa, se genera dentro y fuera de ella. El control de la informaci\u00f3n generada y la naturaleza de la misma ha dejado de ser centralizada en la empresa. Capturar, procesar, entender y actuar en consecuencia, permite a las empresas comprender r\u00e1pidamente el entorno en que se mueven en la \u00e1rea de las redes sociales y tener una ventaja competitiva respecto a sus rivales. Big data describe un conjunto de datos o combinaci\u00f3n de los mismos que por su naturaleza, excede la capacidad de procesamiento y almacenamiento de los sistemas y bases de datos convencionales. Las 5 Vs \u00b6 Ejercicio_1 Busca informaci\u00f3n sobre m\u00e1s Vs en el mundo del Big Data. Significado de Big Data \u00b6 Referencia a un conjunto masivo de datos. Referencia a t\u00e9cnicas y herramientas inform\u00e1ticas para el almacenamiento de una gran cantidad de datos. Referencia a t\u00e9cnicas y herramientas inform\u00e1ticas para procesar el flujo de datos y aplicar transformaciones. Referencia a algoritmos de IA y herramientas inform\u00e1ticas. Almacenamiento \u00b6 Procesamiento \u00b6 MAPREDUCE Fases de los proyectos Big Data \u00b6 1. Establecer el objetivo de la investigaci\u00f3n Cuando se realiza un proyecto de ciencia de datos, tenemos que tener claro el objetivo y caracter\u00edsticas como qu\u00e9 se va a investigar, c\u00f3mo se beneficia la empresa, qu\u00e9 datos y recursos necesita, qu\u00e9 calendario de entrega tiene, etc. - Definir el objetivo de la investigaci\u00f3n. - Crear el estatuto del proyecto. En el estatuto del proyecto tiene que contener lo siguiente: a. Un objetivo de investigaci\u00f3n claro. b. La misi\u00f3n y el contexto del proyecto. c. C\u00f3mo se va a realizar el an\u00e1lisis. d. Qu\u00e9 recursos se van a utilizar. e. Prueba de que es un proyecto alcanzable. f. Una medida de \u00e9xito. g. Una l\u00ednea de tiempo. 2. Recopilar datos Recolecci\u00f3n de datos tanto de la empresa como de terceros verificando la existencia, la calidad y el acceso a los datos. El objetivo es adquirir todos los datos que se necesitan. Esto puede ser dif\u00edcil. Los datos suelen ser como un diamante en bruto que es necesario pulir para que sirva de algo. - Datos internos - Datos externos 3. Preparaci\u00f3n de datos La recopilaci\u00f3n de datos, puede llevar a datos err\u00f3neos, por lo tanto, en esta fase se mejora la calidad de los datos y se preparan para su uso en las siguientes fases. Se realiza una limpieza de datos para asegurar que los datos est\u00e9n en un formato adecuado para utilizar en sus modelos. Esta fase tiene 3 etapas: Limpieza de datos: eliminar errores en sus datos para que sus datos se conviertan en una representaci\u00f3n verdadera y consistente de los procesos de los que se originan. Ejemplo: Soluci\u00f3n: Combinaci\u00f3n de datos diferentes: Los datos provienen de varios lugares diferentes y por lo tanto tenemos que realizar diferentes operaciones (unir, agregar o apilar, ...) para crear o bien otra tabla o bien una vista que no consume espacio en disco. Transformaci\u00f3n de datos: Algunos modelos requieren que sus datos tengan una forma determinada. Una vez que los datos se han limpiado y se han integrado, la siguiente tarea ser\u00e1 transformar sus datos para que adopten una forma adecuada para el modelo de datos. 4. Exploraci\u00f3n de datos Para la comprensi\u00f3n m\u00e1s profunda de los datos. Se intenta comprender c\u00f3mo las variables interact\u00faan entre s\u00ed, la distribuci\u00f3n de los datos y si existen valores at\u00edpicos. Para lograr esto, se utilizan principalmente estad\u00edsticas descriptivas, t\u00e9cnicas visuales y modelado simple. La informaci\u00f3n es m\u00e1s f\u00e1cil de captar cuando se muestra en una imagen, por lo tanto, se utilizan principalmente t\u00e9cnicas gr\u00e1ficas para comprender los datos y las interacciones entre variables. Aunque el objetivo no es limpiar datos, en esta fase se pueden descubrir anomal\u00edas que se nos hayan escapado antes, lo que obligar\u00e1 a dar un paso atr\u00e1s y corregirlas. Las t\u00e9cnicas de visualizaci\u00f3n que se utilizan en esta fase van desde simples gr\u00e1ficos lineales, hasta complejos gr\u00e1ficos de red. 5. Modelado de datos o construcci\u00f3n de modelos Una vez que tengo los datos limpios en su lugar y una buena comprensi\u00f3n del contenido, ya estamos en disposici\u00f3n de construir modelos con el objetivo de hacer mejores predicciones, clasificar objetos o comprender el sistema que est\u00e1 modelando. En esta fase se utilizan modelos de conocimientos sobre los datos que se encontraron en la fase anterior para poder responder a la fase de investigaci\u00f3n. Se seleccionan t\u00e9cnicas de los campos de la estad\u00edstica, aprendizaje autom\u00e1tico, investigaci\u00f3n de operaciones, etc. La construcci\u00f3n de un modelo es un proceso iterativo que implica la selecci\u00f3n de las variables para el modelo, la ejecuci\u00f3n y los diagn\u00f3sticos del modelo. Las t\u00e9cnicas que se usan provienen de: Machine Learning (Aprendizaje autom\u00e1tico) Miner\u00eda de datos La estad\u00edstica Ejemplo Caso de estudio: \u00bfQu\u00e9 productos se compran usualmente juntos? . El conjunto de datos contiene, por cada compra de un cliente, qu\u00e9 productos adquiri\u00f3 y cu\u00e1l fue el precio final de la compra. Aplicando miner\u00eda de datos y en concreto t\u00e9cnicas de Reglas de Asociaci\u00f3n se obtienen los siguientes resultados: 6. Presentaci\u00f3n y automatizaci\u00f3n Despu\u00e9s de haber analizado con \u00e9xito los datos y construido un modelo de buen rendimiento, se presentan los resultados de diferentes formas, desde presentaciones hasta informes de investigaci\u00f3n. Ejercicio_2 Una vez vistas las fases del proceso de ciencia de datos. Busca o inventa un proyecto que una empresa podr\u00eda pedirnos y analiza cada una de las fases que tendr\u00eda que tener el proyecto. Tipos de almacenamientos \u00b6 ON-PREMISE - Cuando la informaci\u00f3n se almacena en servidores, racks locales, instalados en la propia organizaci\u00f3n. CLOUD - NUBE - Cuando se utiliza las infraestructuras de terceros (AWS, Google, ...). Tiene las siguientes caracter\u00edsticas: a. Capacidad de abstracci\u00f3n de los recursos. b. Escalabilidad. c. Modelo de servicio pago por uso. d. Acceso ubicuo. NUBE P\u00daBLICA - Los datos se guardan en una nube que est\u00e1 abierta al uso de todas las personas que lo deseen. NUBE PRIVADA - Los datos se guardan en una nube, pero dentro de un entorno local de dif\u00edcil acceso a todos aquellos que no sean de la empresa. HYBRID CLOUD - Cuando se produce una combinaci\u00f3n entre la nube p\u00fablica y la nube privada. EDGE - El almacenamiento y procesamiento de la informaci\u00f3n se produce cerca del punto de recolecci\u00f3n. Se ubican unas zonas intermedias que nos permiten procesar y almacenar datos mucho m\u00e1s cerca de los dispositivos por lo que la velocidad se dispara y la latencia se reduce. Modelo Cloud Federadas Son proveedores m\u00e1s peque\u00f1os que se agrupan para ofrecer al cliente diferentes servicios. Se federan para optimizar los recursos. Un cliente lanza una petici\u00f3n a la nube y le llega a un broker que ser\u00e1 quien decida a que proveedor se lo env\u00eda. Si un proveedor est\u00e1 saturado, no se le env\u00edan peticiones e ir\u00edan a otro proveedor. Para el cliente, \u00e9sto es transparente. Los grandes proveedores no entran en esto (porque no lo necesitan). La uni\u00f3n europea ha iniciado un proyecto llamado proyecto Gaia-X. Ejercicio_ampliaci\u00f3n Investiga sobre el proyecto Gaia-X","title":"2.0. IAW"},{"location":"apuntes01.html#big-data","text":"Sabias que ... \"Cada dos d\u00edas creamos tanta informaci\u00f3n como la que se cre\u00f3 desde el amanecer de la civilizaci\u00f3n hasta 2003\" Erick Schimidt- CEO Google","title":"Big Data"},{"location":"apuntes01.html#introduccion","text":"El gran volumen de datos que ha comenzado a generarse en la \u00faltima d\u00e9cada debido a la capacidad de almacenamiento y procesamiento surgida por nuevas tecnolog\u00edas. La informaci\u00f3n referente a una empresa, se genera dentro y fuera de ella. El control de la informaci\u00f3n generada y la naturaleza de la misma ha dejado de ser centralizada en la empresa. Capturar, procesar, entender y actuar en consecuencia, permite a las empresas comprender r\u00e1pidamente el entorno en que se mueven en la \u00e1rea de las redes sociales y tener una ventaja competitiva respecto a sus rivales. Big data describe un conjunto de datos o combinaci\u00f3n de los mismos que por su naturaleza, excede la capacidad de procesamiento y almacenamiento de los sistemas y bases de datos convencionales.","title":"Introducci\u00f3n"},{"location":"apuntes01.html#las-5-vs","text":"Ejercicio_1 Busca informaci\u00f3n sobre m\u00e1s Vs en el mundo del Big Data.","title":"Las 5 Vs"},{"location":"apuntes01.html#significado-de-big-data","text":"Referencia a un conjunto masivo de datos. Referencia a t\u00e9cnicas y herramientas inform\u00e1ticas para el almacenamiento de una gran cantidad de datos. Referencia a t\u00e9cnicas y herramientas inform\u00e1ticas para procesar el flujo de datos y aplicar transformaciones. Referencia a algoritmos de IA y herramientas inform\u00e1ticas.","title":"Significado de Big Data"},{"location":"apuntes01.html#almacenamiento","text":"","title":"Almacenamiento"},{"location":"apuntes01.html#procesamiento","text":"MAPREDUCE","title":"Procesamiento"},{"location":"apuntes01.html#fases-de-los-proyectos-big-data","text":"1. Establecer el objetivo de la investigaci\u00f3n Cuando se realiza un proyecto de ciencia de datos, tenemos que tener claro el objetivo y caracter\u00edsticas como qu\u00e9 se va a investigar, c\u00f3mo se beneficia la empresa, qu\u00e9 datos y recursos necesita, qu\u00e9 calendario de entrega tiene, etc. - Definir el objetivo de la investigaci\u00f3n. - Crear el estatuto del proyecto. En el estatuto del proyecto tiene que contener lo siguiente: a. Un objetivo de investigaci\u00f3n claro. b. La misi\u00f3n y el contexto del proyecto. c. C\u00f3mo se va a realizar el an\u00e1lisis. d. Qu\u00e9 recursos se van a utilizar. e. Prueba de que es un proyecto alcanzable. f. Una medida de \u00e9xito. g. Una l\u00ednea de tiempo. 2. Recopilar datos Recolecci\u00f3n de datos tanto de la empresa como de terceros verificando la existencia, la calidad y el acceso a los datos. El objetivo es adquirir todos los datos que se necesitan. Esto puede ser dif\u00edcil. Los datos suelen ser como un diamante en bruto que es necesario pulir para que sirva de algo. - Datos internos - Datos externos 3. Preparaci\u00f3n de datos La recopilaci\u00f3n de datos, puede llevar a datos err\u00f3neos, por lo tanto, en esta fase se mejora la calidad de los datos y se preparan para su uso en las siguientes fases. Se realiza una limpieza de datos para asegurar que los datos est\u00e9n en un formato adecuado para utilizar en sus modelos. Esta fase tiene 3 etapas: Limpieza de datos: eliminar errores en sus datos para que sus datos se conviertan en una representaci\u00f3n verdadera y consistente de los procesos de los que se originan. Ejemplo: Soluci\u00f3n: Combinaci\u00f3n de datos diferentes: Los datos provienen de varios lugares diferentes y por lo tanto tenemos que realizar diferentes operaciones (unir, agregar o apilar, ...) para crear o bien otra tabla o bien una vista que no consume espacio en disco. Transformaci\u00f3n de datos: Algunos modelos requieren que sus datos tengan una forma determinada. Una vez que los datos se han limpiado y se han integrado, la siguiente tarea ser\u00e1 transformar sus datos para que adopten una forma adecuada para el modelo de datos. 4. Exploraci\u00f3n de datos Para la comprensi\u00f3n m\u00e1s profunda de los datos. Se intenta comprender c\u00f3mo las variables interact\u00faan entre s\u00ed, la distribuci\u00f3n de los datos y si existen valores at\u00edpicos. Para lograr esto, se utilizan principalmente estad\u00edsticas descriptivas, t\u00e9cnicas visuales y modelado simple. La informaci\u00f3n es m\u00e1s f\u00e1cil de captar cuando se muestra en una imagen, por lo tanto, se utilizan principalmente t\u00e9cnicas gr\u00e1ficas para comprender los datos y las interacciones entre variables. Aunque el objetivo no es limpiar datos, en esta fase se pueden descubrir anomal\u00edas que se nos hayan escapado antes, lo que obligar\u00e1 a dar un paso atr\u00e1s y corregirlas. Las t\u00e9cnicas de visualizaci\u00f3n que se utilizan en esta fase van desde simples gr\u00e1ficos lineales, hasta complejos gr\u00e1ficos de red. 5. Modelado de datos o construcci\u00f3n de modelos Una vez que tengo los datos limpios en su lugar y una buena comprensi\u00f3n del contenido, ya estamos en disposici\u00f3n de construir modelos con el objetivo de hacer mejores predicciones, clasificar objetos o comprender el sistema que est\u00e1 modelando. En esta fase se utilizan modelos de conocimientos sobre los datos que se encontraron en la fase anterior para poder responder a la fase de investigaci\u00f3n. Se seleccionan t\u00e9cnicas de los campos de la estad\u00edstica, aprendizaje autom\u00e1tico, investigaci\u00f3n de operaciones, etc. La construcci\u00f3n de un modelo es un proceso iterativo que implica la selecci\u00f3n de las variables para el modelo, la ejecuci\u00f3n y los diagn\u00f3sticos del modelo. Las t\u00e9cnicas que se usan provienen de: Machine Learning (Aprendizaje autom\u00e1tico) Miner\u00eda de datos La estad\u00edstica Ejemplo Caso de estudio: \u00bfQu\u00e9 productos se compran usualmente juntos? . El conjunto de datos contiene, por cada compra de un cliente, qu\u00e9 productos adquiri\u00f3 y cu\u00e1l fue el precio final de la compra. Aplicando miner\u00eda de datos y en concreto t\u00e9cnicas de Reglas de Asociaci\u00f3n se obtienen los siguientes resultados: 6. Presentaci\u00f3n y automatizaci\u00f3n Despu\u00e9s de haber analizado con \u00e9xito los datos y construido un modelo de buen rendimiento, se presentan los resultados de diferentes formas, desde presentaciones hasta informes de investigaci\u00f3n. Ejercicio_2 Una vez vistas las fases del proceso de ciencia de datos. Busca o inventa un proyecto que una empresa podr\u00eda pedirnos y analiza cada una de las fases que tendr\u00eda que tener el proyecto.","title":"Fases de los proyectos Big Data"},{"location":"apuntes01.html#tipos-de-almacenamientos","text":"ON-PREMISE - Cuando la informaci\u00f3n se almacena en servidores, racks locales, instalados en la propia organizaci\u00f3n. CLOUD - NUBE - Cuando se utiliza las infraestructuras de terceros (AWS, Google, ...). Tiene las siguientes caracter\u00edsticas: a. Capacidad de abstracci\u00f3n de los recursos. b. Escalabilidad. c. Modelo de servicio pago por uso. d. Acceso ubicuo. NUBE P\u00daBLICA - Los datos se guardan en una nube que est\u00e1 abierta al uso de todas las personas que lo deseen. NUBE PRIVADA - Los datos se guardan en una nube, pero dentro de un entorno local de dif\u00edcil acceso a todos aquellos que no sean de la empresa. HYBRID CLOUD - Cuando se produce una combinaci\u00f3n entre la nube p\u00fablica y la nube privada. EDGE - El almacenamiento y procesamiento de la informaci\u00f3n se produce cerca del punto de recolecci\u00f3n. Se ubican unas zonas intermedias que nos permiten procesar y almacenar datos mucho m\u00e1s cerca de los dispositivos por lo que la velocidad se dispara y la latencia se reduce. Modelo Cloud Federadas Son proveedores m\u00e1s peque\u00f1os que se agrupan para ofrecer al cliente diferentes servicios. Se federan para optimizar los recursos. Un cliente lanza una petici\u00f3n a la nube y le llega a un broker que ser\u00e1 quien decida a que proveedor se lo env\u00eda. Si un proveedor est\u00e1 saturado, no se le env\u00edan peticiones e ir\u00edan a otro proveedor. Para el cliente, \u00e9sto es transparente. Los grandes proveedores no entran en esto (porque no lo necesitan). La uni\u00f3n europea ha iniciado un proyecto llamado proyecto Gaia-X. Ejercicio_ampliaci\u00f3n Investiga sobre el proyecto Gaia-X","title":"Tipos de almacenamientos"},{"location":"apuntes02.html","text":"PERFILES \u00b6 COMPARACI\u00d3N DE PERFILES \u00b6 Los diferentes perfiles son: Analista de datos: Transforma los datos en informaci\u00f3n. Realiza un an\u00e1lisis m\u00e1s tradicional de los datos, los transforma y los interpreta con el objetivo de dar respuesta a los retos propuestos desde la inteligencia de negocio. Ingeniero de datos: Es el perfil m\u00e1s t\u00e9cnico. Profesional enfocado en el dise\u00f1o, desarrollo y mantenimiento de los procesos necesarios para la extracci\u00f3n, carga, almacenamiento y procesamiento de los datos (ETL).Es un experto en ingenier\u00eda del software. Cient\u00edfico de datos: Es la profesi\u00f3n m\u00e1s demandada. Su funci\u00f3n es similar a la del Analista de datos pero su rol se ubica en las fuentes de informaci\u00f3n masivas y complejas. Examina los datos, los transforma y analiza con el objetivo de extraer informaci\u00f3n y convertirla en conocimiento. Se encargan del entrenamiento de modelos de IA. Arquitecto de datos: Dise\u00f1a arquitecturas que pemiten la gesti\u00f3n de grandes vol\u00famenes de datos que no pueden ser tratados de manera convencional. Se encarga de definir la estrategia que se aplicar\u00e1 con respecto a la gobernanza y seguridad de los datos.Un arquitecto de datos define las herramientas y la arquitectura en la que se almacenar\u00edan los datos, mientras que un cient\u00edfico de datos utiliza esta arquitectura. Ejercicio_3 Examina diferentes ofertas de trabajo de diferentes ciudades y explica los perfiles m\u00e1s demandados as\u00ed como las herramientas m\u00e1s utilizadas.","title":"2.1. PERFILES"},{"location":"apuntes02.html#perfiles","text":"","title":"PERFILES"},{"location":"apuntes02.html#comparacion-de-perfiles","text":"Los diferentes perfiles son: Analista de datos: Transforma los datos en informaci\u00f3n. Realiza un an\u00e1lisis m\u00e1s tradicional de los datos, los transforma y los interpreta con el objetivo de dar respuesta a los retos propuestos desde la inteligencia de negocio. Ingeniero de datos: Es el perfil m\u00e1s t\u00e9cnico. Profesional enfocado en el dise\u00f1o, desarrollo y mantenimiento de los procesos necesarios para la extracci\u00f3n, carga, almacenamiento y procesamiento de los datos (ETL).Es un experto en ingenier\u00eda del software. Cient\u00edfico de datos: Es la profesi\u00f3n m\u00e1s demandada. Su funci\u00f3n es similar a la del Analista de datos pero su rol se ubica en las fuentes de informaci\u00f3n masivas y complejas. Examina los datos, los transforma y analiza con el objetivo de extraer informaci\u00f3n y convertirla en conocimiento. Se encargan del entrenamiento de modelos de IA. Arquitecto de datos: Dise\u00f1a arquitecturas que pemiten la gesti\u00f3n de grandes vol\u00famenes de datos que no pueden ser tratados de manera convencional. Se encarga de definir la estrategia que se aplicar\u00e1 con respecto a la gobernanza y seguridad de los datos.Un arquitecto de datos define las herramientas y la arquitectura en la que se almacenar\u00edan los datos, mientras que un cient\u00edfico de datos utiliza esta arquitectura. Ejercicio_3 Examina diferentes ofertas de trabajo de diferentes ciudades y explica los perfiles m\u00e1s demandados as\u00ed como las herramientas m\u00e1s utilizadas.","title":"COMPARACI\u00d3N DE PERFILES"},{"location":"apuntes03.html","text":"TIPOS DE DATOS \u00b6 Estructurados \u00b6 Los datos estructurados son datos que dependen de un modelo de datos y se encuentran en un campo fijo dentro de un registro. No estructurados \u00b6 Los datos no estructurados no encajan en un modelo de datos espec\u00edfico. Un ejemplo de datos no estructurado puede ser un correo electr\u00f3nico habitual. Aunque el correo electr\u00f3nico contiene elementos estructurados como el remitente, el t\u00edtulo y el cuerpo del texto, el contenido del cuerpo es lo que contiene datos no estructurados. Lenguaje Natural \u00b6 El lenguaje natural es un tipo especial de datos no estructurados; es un desaf\u00edo ya que para ser procesado se requiere de conocimiento de t\u00e9cnicas espec\u00edficas de ciencia de datos y ling\u00fc\u00edstica. El procesamiento del lenguaje natural ha tenido \u00e9xito en el reconocimiento de entidades, de temas, resumen, finalizaci\u00f3n de texto y an\u00e1lisis de sentimientos, pero los modelos entrenados en un dominio no generalizan bien a otros dominios. Generado por m\u00e1quinas \u00b6 Los datos generados por la m\u00e1quina son informaci\u00f3n que es creada autom\u00e1ticamente por un ordenador, proceso, aplicaci\u00f3n u otra m\u00e1quina sin intervenci\u00f3n humana. Ejemplos de datos de la m\u00e1quina son los registros del servidor web, los registros de detalles de llamada, los registros de eventos de red y la telemetr\u00eda. Basados en gr\u00e1ficos \u00b6 Un grafo es una estructura matem\u00e1tica para modelar relaciones por pares entre objetos. Las estructuras de los gr\u00e1ficos utilizan nodos, bordes y propiedades para representar y almacenar datos gr\u00e1ficos. Los datos basados en gr\u00e1ficos son una forma natural de representar las redes sociales, y su estructura le permite calcular m\u00e9tricas espec\u00edficas como por ejemplo el camino m\u00e1s corto entre dos personas.La lista de seguidores en Twitter es otro ejemplo de datos basados en gr\u00e1ficos. Audio, v\u00eddeo e im\u00e1genes \u00b6 El audio, la imagen y el v\u00eddeo son tipos de datos que plantean desaf\u00edos espec\u00edficos para un cient\u00edfico de datos. Las tareas que son triviales para los humanos, como reconocer objetos en im\u00e1genes, resultan ser un desaf\u00edo para los ordenadores. Recientemente, una empresa llamada DeepMind logr\u00f3 crear un algoritmo capaz de aprender a jugar a videojuegos. Este algoritmo toma la pantalla de v\u00eddeo como entrada y aprende a interpretar todo a trav\u00e9s de un complejo proceso de deep learning. Es una haza\u00f1a que llev\u00f3 a Google a comprar la empresa para sus propios planes de desarrollo de Inteligencia Artificial. Streaming \u00b6 Los datos en streaming fluyen hacia el sistema cuando ocurre un evento en lugar de cargarse en un almac\u00e9n de datos en un lote. Algunos ejemplos son las \"tendencias\" en Twitter, eventos deportivos o musicales en vivo y el mercado de valores.","title":"2.2. TIPOS DE DATOS"},{"location":"apuntes03.html#tipos-de-datos","text":"","title":"TIPOS DE DATOS"},{"location":"apuntes03.html#estructurados","text":"Los datos estructurados son datos que dependen de un modelo de datos y se encuentran en un campo fijo dentro de un registro.","title":"Estructurados"},{"location":"apuntes03.html#no-estructurados","text":"Los datos no estructurados no encajan en un modelo de datos espec\u00edfico. Un ejemplo de datos no estructurado puede ser un correo electr\u00f3nico habitual. Aunque el correo electr\u00f3nico contiene elementos estructurados como el remitente, el t\u00edtulo y el cuerpo del texto, el contenido del cuerpo es lo que contiene datos no estructurados.","title":"No estructurados"},{"location":"apuntes03.html#lenguaje-natural","text":"El lenguaje natural es un tipo especial de datos no estructurados; es un desaf\u00edo ya que para ser procesado se requiere de conocimiento de t\u00e9cnicas espec\u00edficas de ciencia de datos y ling\u00fc\u00edstica. El procesamiento del lenguaje natural ha tenido \u00e9xito en el reconocimiento de entidades, de temas, resumen, finalizaci\u00f3n de texto y an\u00e1lisis de sentimientos, pero los modelos entrenados en un dominio no generalizan bien a otros dominios.","title":"Lenguaje Natural"},{"location":"apuntes03.html#generado-por-maquinas","text":"Los datos generados por la m\u00e1quina son informaci\u00f3n que es creada autom\u00e1ticamente por un ordenador, proceso, aplicaci\u00f3n u otra m\u00e1quina sin intervenci\u00f3n humana. Ejemplos de datos de la m\u00e1quina son los registros del servidor web, los registros de detalles de llamada, los registros de eventos de red y la telemetr\u00eda.","title":"Generado por m\u00e1quinas"},{"location":"apuntes03.html#basados-en-graficos","text":"Un grafo es una estructura matem\u00e1tica para modelar relaciones por pares entre objetos. Las estructuras de los gr\u00e1ficos utilizan nodos, bordes y propiedades para representar y almacenar datos gr\u00e1ficos. Los datos basados en gr\u00e1ficos son una forma natural de representar las redes sociales, y su estructura le permite calcular m\u00e9tricas espec\u00edficas como por ejemplo el camino m\u00e1s corto entre dos personas.La lista de seguidores en Twitter es otro ejemplo de datos basados en gr\u00e1ficos.","title":"Basados en gr\u00e1ficos"},{"location":"apuntes03.html#audio-video-e-imagenes","text":"El audio, la imagen y el v\u00eddeo son tipos de datos que plantean desaf\u00edos espec\u00edficos para un cient\u00edfico de datos. Las tareas que son triviales para los humanos, como reconocer objetos en im\u00e1genes, resultan ser un desaf\u00edo para los ordenadores. Recientemente, una empresa llamada DeepMind logr\u00f3 crear un algoritmo capaz de aprender a jugar a videojuegos. Este algoritmo toma la pantalla de v\u00eddeo como entrada y aprende a interpretar todo a trav\u00e9s de un complejo proceso de deep learning. Es una haza\u00f1a que llev\u00f3 a Google a comprar la empresa para sus propios planes de desarrollo de Inteligencia Artificial.","title":"Audio, v\u00eddeo e im\u00e1genes"},{"location":"apuntes03.html#streaming","text":"Los datos en streaming fluyen hacia el sistema cuando ocurre un evento en lugar de cargarse en un almac\u00e9n de datos en un lote. Algunos ejemplos son las \"tendencias\" en Twitter, eventos deportivos o musicales en vivo y el mercado de valores.","title":"Streaming"},{"location":"apuntes04.html","text":"FORMATO DE DATOS \u00b6 Datos abiertos \u00b6 Son datos libremente accesibles y reutilizables, con la \u00fanica condici\u00f3n de la atribuci\u00f3n (mencionar la organizaci\u00f3n). Tienen unas caracter\u00edsticas legales de licencia, protecci\u00f3n de datos, etc. Tabla de proveedores de datos abiertos En 2010, el experto brit\u00e1nico Tim Berners-Lee formul\u00f3 el modelo 5 estrellas para animar a la sociedad, en particular a los encargados de los datos gubernamentales, a abrir sus bases de datos, pretende favorece la reutilizaci\u00f3n de los datos. Este modelo puntua los datos seg\u00fan su nivel de reutilizaci\u00f3n y acceso. Datos abiertos del gobierno de Espa\u00f1a Formato de datos \u00b6 CSV \u00b6 Comma Separated Values (valores separados por comas). Requiere que cada elemento de nuestro conjunto se presente en una l\u00ednea. Dentro de esa l\u00ednea, cada uno de los atributos del elemento debe estar separado por un \u00fanico separador, que habitualmente es una coma, y seguir siempre el mismo orden. Adem\u00e1s, la primera l\u00ednea del fichero, a la que llamaremos cabecera , no contiene datos de ning\u00fan elemento, sino informaci\u00f3n de los atributos. Si el campo contiene alguna coma, utilizaremos un delimitador como por ejemplo \" \". XML \u00b6 Extensive Markup Languaje (lenguaje de marcas extensible). Es un lenguaje de etiquetas utilizado para almacenar datos de forma estructurada. JSON \u00b6 JavaScript Object Notation, es un formato muy utilizado hoy en d\u00eda, tiene el mismo prop\u00f3sito que el XML que es el intercambio de datos pero no utiliza las etiquetas abiertas y cerradas, sino que pretende que pese menos, es decir que ocupe menos espacio. Avro \u00b6 Es un formato de almacenamiento basado en filas para Hadoop. Avro se basa en esquemas . Cuando los datos .avro son le\u00eddos siempre est\u00e1 presente el esquema con el que han sido escritos. Avro utiliza JSON para definir tipos de datos y protocolos. Es el formato utilizado para la serializaci\u00f3n de datos ya que es m\u00e1s r\u00e1pido y ocupa menos espacio que los JSON, la serializaci\u00f3n de los datos la hace en un formato binario compacto. Parquet \u00b6 Es un formato de almacenamiento basado en columnas para Hadoop. Fue creado para poder disponer de un formato de compresi\u00f3n y codificaci\u00f3n eficiente. El formato Parquet est\u00e1 compuesto por tres piezas: Row group: es un conjunto de filas en formato columnar. Column chunk: son los datos de una columna en grupo. Se puede leer de manera independiente para mejorar las lecturas. Page: es donde finalmente se almacenan los datos, debe ser lo suficientemente grade para que la compresi\u00f3n sea eficiente.","title":"2.3. FORMATO DE DATOS"},{"location":"apuntes04.html#formato-de-datos","text":"","title":"FORMATO DE DATOS"},{"location":"apuntes04.html#datos-abiertos","text":"Son datos libremente accesibles y reutilizables, con la \u00fanica condici\u00f3n de la atribuci\u00f3n (mencionar la organizaci\u00f3n). Tienen unas caracter\u00edsticas legales de licencia, protecci\u00f3n de datos, etc. Tabla de proveedores de datos abiertos En 2010, el experto brit\u00e1nico Tim Berners-Lee formul\u00f3 el modelo 5 estrellas para animar a la sociedad, en particular a los encargados de los datos gubernamentales, a abrir sus bases de datos, pretende favorece la reutilizaci\u00f3n de los datos. Este modelo puntua los datos seg\u00fan su nivel de reutilizaci\u00f3n y acceso. Datos abiertos del gobierno de Espa\u00f1a","title":"Datos abiertos"},{"location":"apuntes04.html#formato-de-datos_1","text":"","title":"Formato de datos"},{"location":"apuntes04.html#csv","text":"Comma Separated Values (valores separados por comas). Requiere que cada elemento de nuestro conjunto se presente en una l\u00ednea. Dentro de esa l\u00ednea, cada uno de los atributos del elemento debe estar separado por un \u00fanico separador, que habitualmente es una coma, y seguir siempre el mismo orden. Adem\u00e1s, la primera l\u00ednea del fichero, a la que llamaremos cabecera , no contiene datos de ning\u00fan elemento, sino informaci\u00f3n de los atributos. Si el campo contiene alguna coma, utilizaremos un delimitador como por ejemplo \" \".","title":"CSV"},{"location":"apuntes04.html#xml","text":"Extensive Markup Languaje (lenguaje de marcas extensible). Es un lenguaje de etiquetas utilizado para almacenar datos de forma estructurada.","title":"XML"},{"location":"apuntes04.html#json","text":"JavaScript Object Notation, es un formato muy utilizado hoy en d\u00eda, tiene el mismo prop\u00f3sito que el XML que es el intercambio de datos pero no utiliza las etiquetas abiertas y cerradas, sino que pretende que pese menos, es decir que ocupe menos espacio.","title":"JSON"},{"location":"apuntes04.html#avro","text":"Es un formato de almacenamiento basado en filas para Hadoop. Avro se basa en esquemas . Cuando los datos .avro son le\u00eddos siempre est\u00e1 presente el esquema con el que han sido escritos. Avro utiliza JSON para definir tipos de datos y protocolos. Es el formato utilizado para la serializaci\u00f3n de datos ya que es m\u00e1s r\u00e1pido y ocupa menos espacio que los JSON, la serializaci\u00f3n de los datos la hace en un formato binario compacto.","title":"Avro"},{"location":"apuntes04.html#parquet","text":"Es un formato de almacenamiento basado en columnas para Hadoop. Fue creado para poder disponer de un formato de compresi\u00f3n y codificaci\u00f3n eficiente. El formato Parquet est\u00e1 compuesto por tres piezas: Row group: es un conjunto de filas en formato columnar. Column chunk: son los datos de una columna en grupo. Se puede leer de manera independiente para mejorar las lecturas. Page: es donde finalmente se almacenan los datos, debe ser lo suficientemente grade para que la compresi\u00f3n sea eficiente.","title":"Parquet"},{"location":"apuntes05.html","text":"RESUMEN \u00b6 Ideas importantes \u00b6 Los datos son el nuevo petr\u00f3leo. Lo m\u00e1s importante es encontrar las variables necesarias para poder hacer un buen an\u00e1lisis. Cualquier compa\u00f1\u00eda se puede beneficiar del Big Data. La correlaci\u00f3n de variables no significa causa-efecto. Todos los proyectos de Big Data tienen un proceso de digitalizaci\u00f3n de datos. Big data implica transformaci\u00f3n. Son proyectos que pueden durar a\u00f1os. La paralelizaci\u00f3n es la t\u00e9cnica utilizada en Big Data que permite reducir el tiempo de computaci\u00f3n de un proceso gracias a dividir el esfuerzo entre varios nodos. La t\u00e9cnica m\u00e1s conocida es MAPREDUCE. Las 5 V's son velocidad, volumen, variedad, valor, veracidad. La nube como m\u00e9todo para el almacenamiento, gesti\u00f3n y procesado de los datos. Diferencia entre Data Warehouse y Data Lake. El proceso de Big Data conlleva el recopilar, almacenar, procesar y analizar, hasta visualizar los datos. CICLO DE VIDA DEL DATO","title":"2.4. RESUMEN"},{"location":"apuntes05.html#resumen","text":"","title":"RESUMEN"},{"location":"apuntes05.html#ideas-importantes","text":"Los datos son el nuevo petr\u00f3leo. Lo m\u00e1s importante es encontrar las variables necesarias para poder hacer un buen an\u00e1lisis. Cualquier compa\u00f1\u00eda se puede beneficiar del Big Data. La correlaci\u00f3n de variables no significa causa-efecto. Todos los proyectos de Big Data tienen un proceso de digitalizaci\u00f3n de datos. Big data implica transformaci\u00f3n. Son proyectos que pueden durar a\u00f1os. La paralelizaci\u00f3n es la t\u00e9cnica utilizada en Big Data que permite reducir el tiempo de computaci\u00f3n de un proceso gracias a dividir el esfuerzo entre varios nodos. La t\u00e9cnica m\u00e1s conocida es MAPREDUCE. Las 5 V's son velocidad, volumen, variedad, valor, veracidad. La nube como m\u00e9todo para el almacenamiento, gesti\u00f3n y procesado de los datos. Diferencia entre Data Warehouse y Data Lake. El proceso de Big Data conlleva el recopilar, almacenar, procesar y analizar, hasta visualizar los datos. CICLO DE VIDA DEL DATO","title":"Ideas importantes"},{"location":"apuntes06.html","text":"VISUALIZACI\u00d3N DE DATOS \u00b6 Colin Ware fue uno de los primeros investigadores en ofrecer una definici\u00f3n de visualizaci\u00f3n de datos. Seg\u00fan Ware, la visualizaci\u00f3n es: \"la representaci\u00f3n gr\u00e1fica de datos o conceptos, que tiene como resultado una imagen mental o un artefacto externo que ayuda en la toma de decisiones\" La visualizaci\u00f3n de datos es una disciplina que explica c\u00f3mo tratar la informaci\u00f3n de un modo visual y que ofrece dos grandes ventajas: El lenguaje visual es el m\u00e1s adecuado para hacer accesibles los datos a p\u00fablico no especialista. La visualizaci\u00f3n facilita destacar cierta informaci\u00f3n en un contexto de sobrecarga informativa. Por lo tanto, la visualizaci\u00f3n de datos es una representaci\u00f3n gr\u00e1fica de datos, pero tambi\u00e9n de conceptos y ayuda a la toma de decisiones. \u00bfQu\u00e9 es la visualizaci\u00f3n de datos? \u00b6 VISUALIZAR PARA EXPLORAR: datos para responder preguntas o plantear nuevas cuestiones que ni siquiera se hab\u00edan imaginado. Contribuyen a abrir un debate sobre un tema y responden a muchas preguntas, a la vez que generan otras nuevas. VISUALIZAR PARA ANALIZAR: patrones, relaciones y valores at\u00edpicos entre los datos. Por lo tanto, primero se puede utilizar la visualizaci\u00f3n para analizar un conjunto de datos y extraer una serie de conclusiones, y despu\u00e9s para explicar una historia o permitir su exploraci\u00f3n seg\u00fan una serie de par\u00e1metros. Cuadro de mando: es una herramienta con la que se puede controlar el estado de un sistema. Por ejemplo, un cuadro de mando financiero permite controlar los ingresos y los gastos de una organizaci\u00f3n por diferentes departamentos y segmentos de productos y clientes. Va dirigido a usuarios expertos en la tem\u00e1tica que cubre. Por lo tanto, es muy importante entender los objetivos y los conocimientos del usuario que lo utilizar\u00e1. Aplicaci\u00f3n de anal\u00edtica visual: requiere de unas capacidades superiores de an\u00e1lisis y exploraci\u00f3n respecto de un cuadro de mando. Podemos hablar de aplicaciones de anal\u00edtica visual siempre que se puedan utilizar t\u00e9cnicas estad\u00edsticas para analizar los datos como por ejemplo, la media, la mediana, agrupaci\u00f3n, etc. VISUALIZAR PARA EXPLICAR: para comunicar y ayudar a entender adecuadamente los datos en el contexto de una historia con datos o de un prop\u00f3sito determinado. No sirve de nada un buen an\u00e1lisis si no se sabe explicar. 1.Infograf\u00eda: es un componente visual que se dirige a un p\u00fablico \u00e1mplio donde no se suele requerir conocimientos previos del tema tratado. 2.Narrativa por desplazamiento (scrollytelling): es una forma de narraci\u00f3n interactiva donde el usuario debe desplazarse (utilizando la barra de desplazamiento, con el rat\u00f3n o deslizando el dedo en un dispositivo t\u00e1ctil) para obtener m\u00e1s informaci\u00f3n. Es un formato exclusivamente pensado para p\u00e1ginas web, generalmente asociado a grandes medios de comunicaci\u00f3n digitales. https://www.bloomberg.com/graphics/2015-whats-warming-the-world/ Ejemplo de narrativa por desplazamiento. 3. Presentaci\u00f3n: con herramientas como PowerPoint o similares se han convertido en un formato habitual de comunicaci\u00f3n en las organizaciones. 4. V\u00eddeo: los gr\u00e1ficos en movimiento (motion charts) se han convertido en un mecanismo habitual para transmitir datos de forma animada. \u00bfC\u00f3mo se elabora una visualizaci\u00f3n? \u00b6 DEFINIENDO UNA ESTRATEGIA Investigar. Definir objetivos. Definir indicadores. PREPARANDO UNOS DATOS Obtener los datos. Darles forma y limpiar. Procesar. DISE\u00d1ANDO UNOS GR\u00c1FICOS Esbozar. Prototipar. Finalizar.","title":"3.1. INTRODUCCI\u00d3N"},{"location":"apuntes06.html#visualizacion-de-datos","text":"Colin Ware fue uno de los primeros investigadores en ofrecer una definici\u00f3n de visualizaci\u00f3n de datos. Seg\u00fan Ware, la visualizaci\u00f3n es: \"la representaci\u00f3n gr\u00e1fica de datos o conceptos, que tiene como resultado una imagen mental o un artefacto externo que ayuda en la toma de decisiones\" La visualizaci\u00f3n de datos es una disciplina que explica c\u00f3mo tratar la informaci\u00f3n de un modo visual y que ofrece dos grandes ventajas: El lenguaje visual es el m\u00e1s adecuado para hacer accesibles los datos a p\u00fablico no especialista. La visualizaci\u00f3n facilita destacar cierta informaci\u00f3n en un contexto de sobrecarga informativa. Por lo tanto, la visualizaci\u00f3n de datos es una representaci\u00f3n gr\u00e1fica de datos, pero tambi\u00e9n de conceptos y ayuda a la toma de decisiones.","title":"VISUALIZACI\u00d3N DE DATOS"},{"location":"apuntes06.html#que-es-la-visualizacion-de-datos","text":"VISUALIZAR PARA EXPLORAR: datos para responder preguntas o plantear nuevas cuestiones que ni siquiera se hab\u00edan imaginado. Contribuyen a abrir un debate sobre un tema y responden a muchas preguntas, a la vez que generan otras nuevas. VISUALIZAR PARA ANALIZAR: patrones, relaciones y valores at\u00edpicos entre los datos. Por lo tanto, primero se puede utilizar la visualizaci\u00f3n para analizar un conjunto de datos y extraer una serie de conclusiones, y despu\u00e9s para explicar una historia o permitir su exploraci\u00f3n seg\u00fan una serie de par\u00e1metros. Cuadro de mando: es una herramienta con la que se puede controlar el estado de un sistema. Por ejemplo, un cuadro de mando financiero permite controlar los ingresos y los gastos de una organizaci\u00f3n por diferentes departamentos y segmentos de productos y clientes. Va dirigido a usuarios expertos en la tem\u00e1tica que cubre. Por lo tanto, es muy importante entender los objetivos y los conocimientos del usuario que lo utilizar\u00e1. Aplicaci\u00f3n de anal\u00edtica visual: requiere de unas capacidades superiores de an\u00e1lisis y exploraci\u00f3n respecto de un cuadro de mando. Podemos hablar de aplicaciones de anal\u00edtica visual siempre que se puedan utilizar t\u00e9cnicas estad\u00edsticas para analizar los datos como por ejemplo, la media, la mediana, agrupaci\u00f3n, etc. VISUALIZAR PARA EXPLICAR: para comunicar y ayudar a entender adecuadamente los datos en el contexto de una historia con datos o de un prop\u00f3sito determinado. No sirve de nada un buen an\u00e1lisis si no se sabe explicar. 1.Infograf\u00eda: es un componente visual que se dirige a un p\u00fablico \u00e1mplio donde no se suele requerir conocimientos previos del tema tratado. 2.Narrativa por desplazamiento (scrollytelling): es una forma de narraci\u00f3n interactiva donde el usuario debe desplazarse (utilizando la barra de desplazamiento, con el rat\u00f3n o deslizando el dedo en un dispositivo t\u00e1ctil) para obtener m\u00e1s informaci\u00f3n. Es un formato exclusivamente pensado para p\u00e1ginas web, generalmente asociado a grandes medios de comunicaci\u00f3n digitales. https://www.bloomberg.com/graphics/2015-whats-warming-the-world/ Ejemplo de narrativa por desplazamiento. 3. Presentaci\u00f3n: con herramientas como PowerPoint o similares se han convertido en un formato habitual de comunicaci\u00f3n en las organizaciones. 4. V\u00eddeo: los gr\u00e1ficos en movimiento (motion charts) se han convertido en un mecanismo habitual para transmitir datos de forma animada.","title":"\u00bfQu\u00e9 es la visualizaci\u00f3n de datos?"},{"location":"apuntes06.html#como-se-elabora-una-visualizacion","text":"DEFINIENDO UNA ESTRATEGIA Investigar. Definir objetivos. Definir indicadores. PREPARANDO UNOS DATOS Obtener los datos. Darles forma y limpiar. Procesar. DISE\u00d1ANDO UNOS GR\u00c1FICOS Esbozar. Prototipar. Finalizar.","title":"\u00bfC\u00f3mo se elabora una visualizaci\u00f3n?"},{"location":"apuntes07.html","text":"AN\u00c1LISIS DE DATOS EXPLORATORIOS - EDA \u00b6 El an\u00e1lisis exploratorio de datos (Exploratory Data Analysis) es una parte muy importante en cualquier an\u00e1lisis. Es el proceso y t\u00e9cnicas para analizar los datos mediante tratamiento estad\u00edstico: Importar, limpiar y validar. Visualizar distribuciones. Explorar relaciones entre variables. Selecci\u00f3n de caracter\u00edsticas. ... Se utiliza en las fases iniciales de todo proyecto de ciencia de datos. Tambi\u00e9n se utiliza para asesorar la calidad de los datos (dependiendo del resultado, puedes decidir corregirlos o manejarlos de manera diferente). El proceso consiste en generar preguntas acerca de los datos y buscar respuestas visualizando, transformando y modelando los datos para as\u00ed obtener resultados o generar nuevas preguntas. No es un proceso formal, no tiene reglas estrictas sino que es libre y no se rige por ninguna regla concreta. El objetivo durante el proceso EDA es desarrollar un entendimiento de los datos utilizando diferentes herramientas. El EDA es un proceso creativo que consiste en formular preguntas de calidad para poder revelar nuevos aspectos de los datos. Tipos de an\u00e1lisis exploratorio \u00b6 Podemos distinguir dos tipos de an\u00e1lisis: 1. Estad\u00edstico: Media, mediana, moda, varianza,... 2. Visual: Histograma, diagrama de dispersi\u00f3n, de caja,... 1. ESTAD\u00cdSTICA DESCRIPTIVA Son t\u00e9cnicas matem\u00e1ticas para resumir o describir conjuntos de datos de manera cuantitativa. Identifica propiedades de los datos, ruido y valores extremos. 2. VISUALIZACI\u00d3N DE DATOS Tendencia central \u00b6 Las columnas pueden tener miles de valores distintos. Un paso b\u00e1sico al explorar los datos es obtener un valor t\u00edpico para cada columna. Tendencia central es la estimaci\u00f3n de d\u00f3nde est\u00e1 localizada la mayor\u00eda de los datos. * Media (mean, average) Medida m\u00e1s com\u00fan, aunque sensible a valores extremos. * Media ponderada (weighted mean, weighted average) \u00datil cuando queremos dar menos peso a alguna de las observaciones. * Media truncada (trimmed mean, truncated mean) - Eliminamos valores extremos (outliers). - Es una medida m\u00e1s robusta (menos sensible a valores extremos) * Mediana (median, 50th percentile) Valor que ocupa la posici\u00f3n central en un conjunto de datos ordenados. Si el conjunto de datos es par, ser\u00e1 la media de los dos que ocupan esa posici\u00f3n. Menos sensible a valores extremos. * Moda (mode) El valor o categor\u00eda que m\u00e1s se repite en un conjunto de datos. Se usa principalmente para datos categoriales. * Coeficiente de asimetr\u00eda (skewness) Permite saber si el conjunto de datos sigue una distribuci\u00f3n normal. Dispersi\u00f3n \u00b6 La tendencia central es una forma de resumir una variable. Otra forma de hacerlo es mediante la dispersi\u00f3n (variabilidad, midiendo si los valores est\u00e1n agrupados o dispersos. Es \u00fatil para identificar valores extremos (outliers). * Rango (range) Es la diferencia entre el valor m\u00e1s grande y el m\u00e1s peque\u00f1o. Es la medida m\u00e1s b\u00e1sica de dispersi\u00f3n. Es muy sensible a valores extremos. * Cuantil (quantile) Puntos tomados a intervalos regulares de una distribuci\u00f3n que la dividen en conjuntos de igual tama\u00f1o. Cuartiles: dividen la distribuci\u00f3n en cuatro partes (0.25, 0.50, 0.75). Percentiles: divide la distribuci\u00f3n en cien partes. - El percentil P es un valor de manera que al menos P por ciento de los valores tienen este valor o menos, y como m\u00e1ximo 100-P por ciento toman este valor o m\u00e1s. - La mediana es lo mismo que el percentil 50. * Rango intercuart\u00edlico (interquartile range, IQR) Medida habitual de variabilidad. Diferencia entre el percentil 75 (Q3) y el percentil (Q1) Muestra el rango cubierto por la mitad central de los datos. Menos sensible a valores extremos que la varianza y la desviaci\u00f3n est\u00e1ndar. * Desviaci\u00f3n media (mean absolute deviation) Distancia promedio entre cada punto y la media. * Varianza (variance, mean-squared-error) Promedio de la desviaci\u00f3n al cuadrado. * Desviaci\u00f3n est\u00e1ndar (standard deviation) Ra\u00edz cuadrada de la varianza. M\u00e1s f\u00e1cil de interpretar que la varianza al estar en la misma escala. Aunque menos intuitiva es m\u00e1s usada que la desviaci\u00f3n media. Especialmente sensible a valores extremos (igual que la varianza).","title":"3.3. AN\u00c1LISIS EXPLORATORIO DE DATOS"},{"location":"apuntes07.html#analisis-de-datos-exploratorios-eda","text":"El an\u00e1lisis exploratorio de datos (Exploratory Data Analysis) es una parte muy importante en cualquier an\u00e1lisis. Es el proceso y t\u00e9cnicas para analizar los datos mediante tratamiento estad\u00edstico: Importar, limpiar y validar. Visualizar distribuciones. Explorar relaciones entre variables. Selecci\u00f3n de caracter\u00edsticas. ... Se utiliza en las fases iniciales de todo proyecto de ciencia de datos. Tambi\u00e9n se utiliza para asesorar la calidad de los datos (dependiendo del resultado, puedes decidir corregirlos o manejarlos de manera diferente). El proceso consiste en generar preguntas acerca de los datos y buscar respuestas visualizando, transformando y modelando los datos para as\u00ed obtener resultados o generar nuevas preguntas. No es un proceso formal, no tiene reglas estrictas sino que es libre y no se rige por ninguna regla concreta. El objetivo durante el proceso EDA es desarrollar un entendimiento de los datos utilizando diferentes herramientas. El EDA es un proceso creativo que consiste en formular preguntas de calidad para poder revelar nuevos aspectos de los datos.","title":"AN\u00c1LISIS DE DATOS EXPLORATORIOS - EDA"},{"location":"apuntes07.html#tipos-de-analisis-exploratorio","text":"Podemos distinguir dos tipos de an\u00e1lisis: 1. Estad\u00edstico: Media, mediana, moda, varianza,... 2. Visual: Histograma, diagrama de dispersi\u00f3n, de caja,... 1. ESTAD\u00cdSTICA DESCRIPTIVA Son t\u00e9cnicas matem\u00e1ticas para resumir o describir conjuntos de datos de manera cuantitativa. Identifica propiedades de los datos, ruido y valores extremos. 2. VISUALIZACI\u00d3N DE DATOS","title":"Tipos de an\u00e1lisis exploratorio"},{"location":"apuntes07.html#tendencia-central","text":"Las columnas pueden tener miles de valores distintos. Un paso b\u00e1sico al explorar los datos es obtener un valor t\u00edpico para cada columna. Tendencia central es la estimaci\u00f3n de d\u00f3nde est\u00e1 localizada la mayor\u00eda de los datos. * Media (mean, average) Medida m\u00e1s com\u00fan, aunque sensible a valores extremos. * Media ponderada (weighted mean, weighted average) \u00datil cuando queremos dar menos peso a alguna de las observaciones. * Media truncada (trimmed mean, truncated mean) - Eliminamos valores extremos (outliers). - Es una medida m\u00e1s robusta (menos sensible a valores extremos) * Mediana (median, 50th percentile) Valor que ocupa la posici\u00f3n central en un conjunto de datos ordenados. Si el conjunto de datos es par, ser\u00e1 la media de los dos que ocupan esa posici\u00f3n. Menos sensible a valores extremos. * Moda (mode) El valor o categor\u00eda que m\u00e1s se repite en un conjunto de datos. Se usa principalmente para datos categoriales. * Coeficiente de asimetr\u00eda (skewness) Permite saber si el conjunto de datos sigue una distribuci\u00f3n normal.","title":"Tendencia central"},{"location":"apuntes07.html#dispersion","text":"La tendencia central es una forma de resumir una variable. Otra forma de hacerlo es mediante la dispersi\u00f3n (variabilidad, midiendo si los valores est\u00e1n agrupados o dispersos. Es \u00fatil para identificar valores extremos (outliers). * Rango (range) Es la diferencia entre el valor m\u00e1s grande y el m\u00e1s peque\u00f1o. Es la medida m\u00e1s b\u00e1sica de dispersi\u00f3n. Es muy sensible a valores extremos. * Cuantil (quantile) Puntos tomados a intervalos regulares de una distribuci\u00f3n que la dividen en conjuntos de igual tama\u00f1o. Cuartiles: dividen la distribuci\u00f3n en cuatro partes (0.25, 0.50, 0.75). Percentiles: divide la distribuci\u00f3n en cien partes. - El percentil P es un valor de manera que al menos P por ciento de los valores tienen este valor o menos, y como m\u00e1ximo 100-P por ciento toman este valor o m\u00e1s. - La mediana es lo mismo que el percentil 50. * Rango intercuart\u00edlico (interquartile range, IQR) Medida habitual de variabilidad. Diferencia entre el percentil 75 (Q3) y el percentil (Q1) Muestra el rango cubierto por la mitad central de los datos. Menos sensible a valores extremos que la varianza y la desviaci\u00f3n est\u00e1ndar. * Desviaci\u00f3n media (mean absolute deviation) Distancia promedio entre cada punto y la media. * Varianza (variance, mean-squared-error) Promedio de la desviaci\u00f3n al cuadrado. * Desviaci\u00f3n est\u00e1ndar (standard deviation) Ra\u00edz cuadrada de la varianza. M\u00e1s f\u00e1cil de interpretar que la varianza al estar en la misma escala. Aunque menos intuitiva es m\u00e1s usada que la desviaci\u00f3n media. Especialmente sensible a valores extremos (igual que la varianza).","title":"Dispersi\u00f3n"},{"location":"apuntes08.html","text":"VISUALIZACI\u00d3N DE DATOS \u00b6 La visualizaci\u00f3n de datos permite resaltar sus principales caracter\u00edsticas. El an\u00e1lisis estad\u00edstico solo se puede llevar a cabo si est\u00e1 bien presentado. Hay diferentes formas de presentar los datos: Textual Tabular Gr\u00e1fica La representaci\u00f3n gr\u00e1fica es la manera m\u00e1s atractiva y f\u00e1cil de entender. Herramientas de visualizaci\u00f3n \u00b6 Python cuenta con numerosas librer\u00edas para hacer visualizaciones como Matplotlib, Seaborn, Folium, Bokeh, Plotly, Altair,... Matplotlib es una de las m\u00e1s importantes, permite crear gr\u00e1ficos de calidad en 2D y exporta visualizaciones a los formatos m\u00e1s comunes (PDF, SVG, JPG, PNG, BMP, GIF,...) Seaborn es una interfaz de alto nivel que funciona sobre Matplotlib y la complementa, permite visualizaciones m\u00e1s atractivas y complejas que son est\u00e9ticamente m\u00e1s agradables. Tipos de visualizaci\u00f3n \u00b6 Existen muchos tipos de diagramas apropiados para diferentes objetivos: Diagrama de barras (ranking) Histograma (distribuci\u00f3n) Diagrama de densidad (distribuci\u00f3n) Diagrama de l\u00edneas (evoluci\u00f3n) Diagrama de dispersi\u00f3n (correlaci\u00f3n) Mapa de calor (correlaci\u00f3n) Diagrama de caja (distribuci\u00f3n y ranking) Diagrama de enjambre (distribuci\u00f3n) Diagrama de viol\u00edn (distribuci\u00f3n) Diagrama de \u00e1rbol (todo-parte) Nube de palabras (ranking) * Diagrama de barras (Bar chart) \u00b6 Son \u00fatiles para visualizar distribuciones de valores discretos o categoriales. Representa gr\u00e1ficamente la comparaci\u00f3n entre categor\u00edas de datos. * Histograma (Histogram) \u00b6 Es un tipo de gr\u00e1fico de barras para visualizar distribuciones de valores num\u00e9ricos. Indica el n\u00famero de observaciones que caen dentro de un rango de valores (bin) * Histograma vs Diagrama de barras \u00b6 * Diagrama de densidad (Density plot) \u00b6 Es una variante del histograma que usa un kernel gaussiano para visualizar los valores. Ofrece una mejor visi\u00f3n de la forma de la distribuci\u00f3n. * Diagrama de l\u00edneas (Line chart) \u00b6 Es muy \u00fatil para visualizar tendencias.Muestra los datos como puntos unidos por l\u00edneas. * Diagrama de dispersi\u00f3n (Scatter plot) \u00b6 Se utilizan para identificar relaciones, patrones o tendencias entre dos valores num\u00e9ricos. Visualiza agrupaciones de datos, identifica outliers y explora correlaciones. Dos atributos est\u00e1n correlacionados si uno implica el otro: Positiva: cuando uno aumenta el otro tambi\u00e9n. Negativa: cuando uno aumenta el otro disminuye. Neutral: no hay correlaci\u00f3n. Tambi\u00e9n podemos encontrar falsas correlaciones: * Mapa de calor (Heatmap) \u00b6 Visualizar datos mediante c\u00f3digos de colores en dos dimensiones. Es \u00fatil para correlaciones. El tono y/o la intensidad indica c\u00f3mo var\u00edan los datos en el espacio. * Diagrama de caja (Box plot) \u00b6 Describe grupos de datos num\u00e9ricos mediante cuartiles. Es \u00fatil para datos que no siguen una distribuci\u00f3n normal. * Diagrama de enjambre (swarmplot) \u00b6 Permite visualizar los puntos directamente cuando tenemos pocas muestras. Se puede usar en combinaci\u00f3n con los diagramas de caja para ver el n\u00famero de muestras. * Diagrama de viol\u00edn (Violin plot) \u00b6 Proporciona la informaci\u00f3n de un diagrama de caja y adem\u00e1s la distribuci\u00f3n de valores. Adecuados cuando tenemos muchos valores y no se pueden visualizar individualmente. * Diagrama de \u00e1rbol (Tree map) \u00b6 Muestra datos jer\u00e1rquicos como un conjunto de rect\u00e1ngulos. Cada grupo se representa como un rect\u00e1ngulo cuya \u00e1rea es proporcional a su valor. Se pueden usar esquemas de color para representar varias dimensiones. * Nube de palabras (Word cloud) \u00b6 Representaci\u00f3n visual de las palabras que conforman un texto. El tama\u00f1o es mayor para las palabras m\u00e1s frecuentes. Pr\u00e1cticas guiadas Matplotlib \u00b6 Visualizaci\u00f3n1 Visualizaci\u00f3n2 Pr\u00e1cticas guiadas Seaborn \u00b6 Visualizaci\u00f3n1 Visualizaci\u00f3n2","title":"3.2. VISUALIZACI\u00d3N DE DATOS"},{"location":"apuntes08.html#visualizacion-de-datos","text":"La visualizaci\u00f3n de datos permite resaltar sus principales caracter\u00edsticas. El an\u00e1lisis estad\u00edstico solo se puede llevar a cabo si est\u00e1 bien presentado. Hay diferentes formas de presentar los datos: Textual Tabular Gr\u00e1fica La representaci\u00f3n gr\u00e1fica es la manera m\u00e1s atractiva y f\u00e1cil de entender.","title":"VISUALIZACI\u00d3N DE DATOS"},{"location":"apuntes08.html#herramientas-de-visualizacion","text":"Python cuenta con numerosas librer\u00edas para hacer visualizaciones como Matplotlib, Seaborn, Folium, Bokeh, Plotly, Altair,... Matplotlib es una de las m\u00e1s importantes, permite crear gr\u00e1ficos de calidad en 2D y exporta visualizaciones a los formatos m\u00e1s comunes (PDF, SVG, JPG, PNG, BMP, GIF,...) Seaborn es una interfaz de alto nivel que funciona sobre Matplotlib y la complementa, permite visualizaciones m\u00e1s atractivas y complejas que son est\u00e9ticamente m\u00e1s agradables.","title":"Herramientas de visualizaci\u00f3n"},{"location":"apuntes08.html#tipos-de-visualizacion","text":"Existen muchos tipos de diagramas apropiados para diferentes objetivos: Diagrama de barras (ranking) Histograma (distribuci\u00f3n) Diagrama de densidad (distribuci\u00f3n) Diagrama de l\u00edneas (evoluci\u00f3n) Diagrama de dispersi\u00f3n (correlaci\u00f3n) Mapa de calor (correlaci\u00f3n) Diagrama de caja (distribuci\u00f3n y ranking) Diagrama de enjambre (distribuci\u00f3n) Diagrama de viol\u00edn (distribuci\u00f3n) Diagrama de \u00e1rbol (todo-parte) Nube de palabras (ranking)","title":"Tipos de visualizaci\u00f3n"},{"location":"apuntes08.html#diagrama-de-barras-bar-chart","text":"Son \u00fatiles para visualizar distribuciones de valores discretos o categoriales. Representa gr\u00e1ficamente la comparaci\u00f3n entre categor\u00edas de datos.","title":"* Diagrama de barras (Bar chart)"},{"location":"apuntes08.html#histograma-histogram","text":"Es un tipo de gr\u00e1fico de barras para visualizar distribuciones de valores num\u00e9ricos. Indica el n\u00famero de observaciones que caen dentro de un rango de valores (bin)","title":"* Histograma (Histogram)"},{"location":"apuntes08.html#histograma-vs-diagrama-de-barras","text":"","title":"* Histograma vs Diagrama de barras"},{"location":"apuntes08.html#diagrama-de-densidad-density-plot","text":"Es una variante del histograma que usa un kernel gaussiano para visualizar los valores. Ofrece una mejor visi\u00f3n de la forma de la distribuci\u00f3n.","title":"* Diagrama de densidad (Density plot)"},{"location":"apuntes08.html#diagrama-de-lineas-line-chart","text":"Es muy \u00fatil para visualizar tendencias.Muestra los datos como puntos unidos por l\u00edneas.","title":"* Diagrama de l\u00edneas (Line chart)"},{"location":"apuntes08.html#diagrama-de-dispersion-scatter-plot","text":"Se utilizan para identificar relaciones, patrones o tendencias entre dos valores num\u00e9ricos. Visualiza agrupaciones de datos, identifica outliers y explora correlaciones. Dos atributos est\u00e1n correlacionados si uno implica el otro: Positiva: cuando uno aumenta el otro tambi\u00e9n. Negativa: cuando uno aumenta el otro disminuye. Neutral: no hay correlaci\u00f3n. Tambi\u00e9n podemos encontrar falsas correlaciones:","title":"* Diagrama de dispersi\u00f3n (Scatter plot)"},{"location":"apuntes08.html#mapa-de-calor-heatmap","text":"Visualizar datos mediante c\u00f3digos de colores en dos dimensiones. Es \u00fatil para correlaciones. El tono y/o la intensidad indica c\u00f3mo var\u00edan los datos en el espacio.","title":"* Mapa de calor (Heatmap)"},{"location":"apuntes08.html#diagrama-de-caja-box-plot","text":"Describe grupos de datos num\u00e9ricos mediante cuartiles. Es \u00fatil para datos que no siguen una distribuci\u00f3n normal.","title":"* Diagrama de caja (Box plot)"},{"location":"apuntes08.html#diagrama-de-enjambre-swarmplot","text":"Permite visualizar los puntos directamente cuando tenemos pocas muestras. Se puede usar en combinaci\u00f3n con los diagramas de caja para ver el n\u00famero de muestras.","title":"* Diagrama de enjambre (swarmplot)"},{"location":"apuntes08.html#diagrama-de-violin-violin-plot","text":"Proporciona la informaci\u00f3n de un diagrama de caja y adem\u00e1s la distribuci\u00f3n de valores. Adecuados cuando tenemos muchos valores y no se pueden visualizar individualmente.","title":"* Diagrama de viol\u00edn (Violin plot)"},{"location":"apuntes08.html#diagrama-de-arbol-tree-map","text":"Muestra datos jer\u00e1rquicos como un conjunto de rect\u00e1ngulos. Cada grupo se representa como un rect\u00e1ngulo cuya \u00e1rea es proporcional a su valor. Se pueden usar esquemas de color para representar varias dimensiones.","title":"* Diagrama de \u00e1rbol (Tree map)"},{"location":"apuntes08.html#nube-de-palabras-word-cloud","text":"Representaci\u00f3n visual de las palabras que conforman un texto. El tama\u00f1o es mayor para las palabras m\u00e1s frecuentes.","title":"* Nube de palabras (Word cloud)"},{"location":"apuntes08.html#practicas-guiadas-matplotlib","text":"Visualizaci\u00f3n1 Visualizaci\u00f3n2","title":"Pr\u00e1cticas guiadas Matplotlib"},{"location":"apuntes08.html#practicas-guiadas-seaborn","text":"Visualizaci\u00f3n1 Visualizaci\u00f3n2","title":"Pr\u00e1cticas guiadas Seaborn"},{"location":"apuntes09.html","text":"PYTHON \u00b6 Lenguaje de programaci\u00f3n orientado a objetos de alto nivel y de prop\u00f3sito general. Se basa en la simpleza y en la facilidad de lectura de c\u00f3digo disminuyendo costes de mantenimiento y facilitando su aprendizaje. Soporta m\u00f3dulos y librer\u00edas que favorecen la reutilizaci\u00f3n de c\u00f3digo. Dispone de potentes estructuras de datos y librer\u00edas enfocadas al an\u00e1lisis de datos. Es un lenguaje interpretado, se ejecuta l\u00ednea a l\u00ednea. No hay un paso de compilaci\u00f3n como en otros lenguajes. Es open source y multiplataforma (windows, Linux, Mac,...) PANDAS \u00b6 Pandas (panel-data) es una librer\u00eda de c\u00f3digo abierto escrito en python que proporciona estructuras de datos y herramientas de an\u00e1lisis de datos de alto rendimiento, r\u00e1pidas y f\u00e1ciles de usar para manipular datos num\u00e9ricos y series de tiempo. Est\u00e1 construida sobre la librer\u00eda NumPy. Caracter\u00edsticas de la librer\u00eda Pandas \u00b6 Define nuevas estructuras de datos basadas en los arrays de la librer\u00eda NumPy pero con nuevas funcionalidades. Permite leer y escribir f\u00e1cilmente ficheros en formato CSV, Excel, etc. Permite acceder a los datos mediante \u00edndices o nombres para filas y columnas. Ofrece m\u00e9todos para reordenar, dividir y combinar conjuntos de datos. Permite trabajar con series temporales. Realiza todas estas operaciones de forma eficiente. Tipos de datos de Pandas \u00b6 Pandas tiene tres estructuras de datos diferentes: Series: Estructura de una dimensi\u00f3n que contiene una secuencia de valores y un array de etiquetas asociadas (el \u00edndice). DataFrame: Estructura de dos dimensiones (tablas). Panel: Estructura de tres dimensiones (cubos). Estas estructuras se construyen a partir de la librer\u00eda Numpy. Caracter\u00edsticas de un Dataframe \u00b6 Almacena una tabla con una colecci\u00f3n ordenada de columnas. Cada columna puede ser de un tipo diferente (num\u00e9rico, cadena, booleano,...) Tiene \u00edndices tanto para las filas como para las columnas. Cada columna es un elemento de tipo Series. Pandas VS NumPy \u00b6 NumPy es una librer\u00eda para c\u00e1lculo num\u00e9rico y procesamiento de arrays multidimensionales. Los c\u00e1lculos usando arrays de NumPy son m\u00e1s r\u00e1pidos que los arrays normales de Python. Permite manejar gran cantidad de datos y es adecuada para c\u00e1lculo matricial. Descargar la librer\u00eda de Pandas \u00b6 import pandas as pd pd . __version__ Series \u00b6 La clase de objetos Series son estructuras similares a los arrays de una dimensi\u00f3n. Son homog\u00e9neas, es decir, sus elementos tienen que ser del mismo tipo, y su tama\u00f1o es inmutable, es decir, no se puede cambiar el n\u00famero de elementos, aunque si su contenido. Creaci\u00f3n de un objeto de la clase Series (constructor) a partir de una lista o tupla Series (data=lista, index=indices, dtype=tipo) : Devuelve un objeto de tipo Series con los valores de lista, las claves especificadas en \u00edndices y el tipo de datos indicado en tipo. Si no se pasa la lista de \u00edndices se utilizan como \u00edndices los enteros del 0 al n-1, done n es el tama\u00f1o de la serie. Si no se pasa el tipo de dato se deduce. s = pd . Series ({ 'Matem\u00e1ticas' : 6.0 , 'Econom\u00eda' : 4.5 , 'Programaci\u00f3n' : 8.5 }) print ( s ) Matem\u00e1ticas 6.0 Econom\u00eda 4.5 Programaci\u00f3n 8.5 dtype: float64 Atributos de la clase Series \u00b6 s.size : Devuelve el n\u00famero de elementos de la serie s. Tambi\u00e9n podemos utilizar len(s). s.index : Devuelve una lista con los nombres de las filas del DataFrame. s.dtype : Devuelve el tipo de datos de los elementos de la serie s. s.values : Devuelve una lista con los valores asociados al \u00edndice. import pandas as pd s = pd . Series ([ 1 , 2 , 2 , 3 , 3 , 3 , 4 , 4 , 4 , 4 ]) print ( s . size ) print ( s . index ) print ( s . values ) print ( s . dtype ) Acceso a elementos de una Serie \u00b6 s = pd . Series ({ 'Matem\u00e1ticas' : 6.0 , 'Econom\u00eda' : 4.5 , 'Programaci\u00f3n' : 8.5 }) print ( s [ 0 ]) # buscar por \u00edndice print ( s [ 1 ]) print () print ( s [ 1 : 3 ]) #el indice funciona como en listas print () print ( s [ 'Econom\u00eda' ]) #buscar por nombre print ( s . Econom\u00eda ) #otra forma de buscar por nombre cuando no tiene espacios print () print ( s [[ 'Programaci\u00f3n' , 'Matem\u00e1ticas' ]]) print () print ( s [ 'Matem\u00e1ticas' : 'Econom\u00eda' ]) #el \u00faltimo \u00edndice es v\u00e1lido print () print ( s . index ) print ( s . values ) M\u00e9todos estad\u00edsticos de la clase Series \u00b6 s.count() : Devuelve el n\u00famero de elementos que no son None ni NaN (no es un n\u00famero) en la serie s. s.sum() : Devuelve la suma de los datos de la serie s cuando los datos son de un tipo num\u00e9rico, o la concatenaci\u00f3n de ellos cuando son del tipo cadena str. s.cumsum() : Devuelve una serie con la suma acumulada de los datos de la serie s cuando los datos son de un tipo num\u00e9rico. s.value_counts() : Devuelve una serie con la frecuencia (n\u00famero de repeticiones) de cada valor de la serie s. s.min() : Devuelve el menor de los datos de la serie s. s.max() : Devuelve el mayor de los datos de la serie s. s.mean() : Devuelve la media de los datos de la serie s cuando los datos son de un tipo num\u00e9rico. s.std() : Devuelve la desviaci\u00f3n t\u00edpica de los datos de la serie s cuando los datos son de un tipo num\u00e9rico. s.describe() : Devuelve una serie con un resumen descriptivo que incluye el n\u00famero de datos, su suma, el m\u00ednimo, el m\u00e1ximo, la media, la desviaci\u00f3n t\u00edpica y los cuartiles. import pandas as pd s = pd . Series ([ 1 , 1 , 1 , 1 , 2 , 2 , 2 , 3 , 3 , 4 ]) print ( s . count ()) print ( s . sum ()) print () print ( s . cumsum ()) print () print ( s . value_counts ()) print ( s . value_counts ( normalize = True ) * 100 ) #tanto por 1 print ( s . min ()) print ( s . max ()) print ( s . mean ()) print ( s . std ()) print () print ( s . describe ()) Ordenar una Serie \u00b6 s.sort_values (ascending=booleano): Devuelve la serie que resulta de ordenar los valores la serie s. Si argumento del par\u00e1metro ascending es True el orden es creciente y si es False decreciente. df.sort_index (ascending=booleano) : Devuelve la serie que resulta de ordenar el \u00edndice de la serie s. Si el argumento del par\u00e1metro ascending es True el orden es creciente y si es False decreciente. import pandas as pd s = pd . Series ({ 'Matem\u00e1ticas' : 6.0 , 'Econom\u00eda' : 4.5 , 'Programaci\u00f3n' : 8.5 }) print ( s . sort_values ()) print () print ( s . sort_index ( ascending = False )) Eliminar los datos desconocidos en una Serie \u00b6 Los datos desconocidos se representan en Pandas por NaN y los nulos por None. Tanto unos como otros suelen ser un problema a la hora de realizar algunos an\u00e1lisis de datos, por lo que es habitual eliminarlos. Para eliminarlos de una serie se utiliza el siguiente m\u00e9todo: s.dropna() : Elimina los datos desconocidos o nulos de la serie s. import pandas as pd import numpy as np s = pd . Series ([ 'a' , 'b' , None , 'c' , np . NaN , 'd' ]) print ( s ) print () print ( s . dropna ()) print () # la serie es inmutable print ( s ) DataFrame \u00b6 Un objeto del tipo DataFrame define un conjunto de datos estructurado en forma de tabla donde cada columna es un objeto de tipo Series, es decir, todos los datos de una misma columna son del mismo tipo, y las filas son registros que pueden contender datos de distintos tipos. Un DataFrame contiene dos \u00edndices, uno para las filas y otro para las columnas, y se puede acceder a sus elementos mediante los nombres de las filas y las columnas. import pandas as pd #cada objeto del diccionario es una columna datos = { 'nombre' :[ 'Mar\u00eda' , 'Luis' , 'Carmen' , 'Antonio' ], 'edad' :[ 18 , 22 , 20 , 21 ], 'grado' :[ 'Econom\u00eda' , 'Medicina' , 'Arquitectura' , 'Econom\u00eda' ], 'correo' :[ 'maria@gmail.com' , 'luis@yahoo.es' , 'carmen@gmail.com' , 'antonio@gmail.com' ] } print ( type ( datos )) print () df = pd . DataFrame ( datos ) print ( type ( df )) print ( df ) df Atributos y m\u00e9todos de un DataFrame \u00b6 df.info() : Devuelve informaci\u00f3n (n\u00famero de filas, n\u00famero de columnas, \u00edndices, tipo de las columnas y memoria usado) sobre el DataFrame df. df.shape : Devuelve una tupla con el n\u00famero de filas y columnas del DataFrame df. df.size : Devuelve el n\u00famero de elementos del DataFrame. len(df) : Defuelve el n\u00famero de filas del DataFrame. df.columns : Devuelve una lista con los nombres de las columnas del DataFrame df. df.index : Devuelve una lista con los nombres de las filas del DataFrame df. df.dtypes : Devuelve una serie con los tipos de datos de las columnas del DataFrame df. df.head(n) : Devuelve las n primeras filas del DataFrame df. df.tail(n) : Devuelve las n \u00faltimas filas del DataFrame df. df.T : Devuelve la traspuesta, cambia filas por columnas. Renombrar los nombres de las filas y las columnas \u00b6 df.rename (columns=columnas, index=filas): Devuelve el DataFrame que resulta de renombrar las columnas indicadas en las claves del diccionario columnas con sus valores y las filas indicadas en las claves del diccionario filas con sus valores en el DataFrame df. df = pd . DataFrame () nombres = [ 'Juan' , 'Laura' , 'Pepe' ] edades = [ 42 , 40 , 37 ] df [ 'Nombre' ] = nombres df [ 'Edad' ] = edades df . rename ( columns = { 'Nombre' : 'Nom' }, index = { 0 : 100 }) print ( df ) Acceso a los elementos de un Dataframe \u00b6 Accesos mediante posiciones: df.iloc[i,j] : Devuelve el elemento que se encuentra en la fila i y la columna j del DataFrame df. Pueden indicarse secuencias de \u00edndices para obtener partes del DataFrame. df.iloc[filas, columnas] : Devuelve un DataFrame con los elementos de las filas de la lista filas y de las columnas de la lista columnas. df.iloc[i] :Devuelve una serie con los elementos de la fila i del DataFrame df. df = pd . DataFrame ([[ 1 , 2 ], [ 4 , 5 ], [ 7 , 8 ]], index = [ 'cobra' , 'viper' , 'sidewinder' ], columns = [ 'max_speed' , 'shield' ]) df df . iloc [ 0 ] df . iloc [[ 0 , 1 ]] df . iloc [: 3 ] Acceso mediante nombres: df.loc[fila, columna] : Devuelve el elemento que se encuentra en la fila con nombre fila y la columna de con nombre columna del DataFrame df. df . loc [ 'viper' ] df . loc [ 'cobra' , 'shield' ]","title":"4.2. PANDAS"},{"location":"apuntes09.html#python","text":"Lenguaje de programaci\u00f3n orientado a objetos de alto nivel y de prop\u00f3sito general. Se basa en la simpleza y en la facilidad de lectura de c\u00f3digo disminuyendo costes de mantenimiento y facilitando su aprendizaje. Soporta m\u00f3dulos y librer\u00edas que favorecen la reutilizaci\u00f3n de c\u00f3digo. Dispone de potentes estructuras de datos y librer\u00edas enfocadas al an\u00e1lisis de datos. Es un lenguaje interpretado, se ejecuta l\u00ednea a l\u00ednea. No hay un paso de compilaci\u00f3n como en otros lenguajes. Es open source y multiplataforma (windows, Linux, Mac,...)","title":"PYTHON"},{"location":"apuntes09.html#pandas","text":"Pandas (panel-data) es una librer\u00eda de c\u00f3digo abierto escrito en python que proporciona estructuras de datos y herramientas de an\u00e1lisis de datos de alto rendimiento, r\u00e1pidas y f\u00e1ciles de usar para manipular datos num\u00e9ricos y series de tiempo. Est\u00e1 construida sobre la librer\u00eda NumPy.","title":"PANDAS"},{"location":"apuntes09.html#caracteristicas-de-la-libreria-pandas","text":"Define nuevas estructuras de datos basadas en los arrays de la librer\u00eda NumPy pero con nuevas funcionalidades. Permite leer y escribir f\u00e1cilmente ficheros en formato CSV, Excel, etc. Permite acceder a los datos mediante \u00edndices o nombres para filas y columnas. Ofrece m\u00e9todos para reordenar, dividir y combinar conjuntos de datos. Permite trabajar con series temporales. Realiza todas estas operaciones de forma eficiente.","title":"Caracter\u00edsticas de la librer\u00eda Pandas"},{"location":"apuntes09.html#tipos-de-datos-de-pandas","text":"Pandas tiene tres estructuras de datos diferentes: Series: Estructura de una dimensi\u00f3n que contiene una secuencia de valores y un array de etiquetas asociadas (el \u00edndice). DataFrame: Estructura de dos dimensiones (tablas). Panel: Estructura de tres dimensiones (cubos). Estas estructuras se construyen a partir de la librer\u00eda Numpy.","title":"Tipos de datos de Pandas"},{"location":"apuntes09.html#caracteristicas-de-un-dataframe","text":"Almacena una tabla con una colecci\u00f3n ordenada de columnas. Cada columna puede ser de un tipo diferente (num\u00e9rico, cadena, booleano,...) Tiene \u00edndices tanto para las filas como para las columnas. Cada columna es un elemento de tipo Series.","title":"Caracter\u00edsticas de un Dataframe"},{"location":"apuntes09.html#pandas-vs-numpy","text":"NumPy es una librer\u00eda para c\u00e1lculo num\u00e9rico y procesamiento de arrays multidimensionales. Los c\u00e1lculos usando arrays de NumPy son m\u00e1s r\u00e1pidos que los arrays normales de Python. Permite manejar gran cantidad de datos y es adecuada para c\u00e1lculo matricial.","title":"Pandas VS NumPy"},{"location":"apuntes09.html#descargar-la-libreria-de-pandas","text":"import pandas as pd pd . __version__","title":"Descargar la librer\u00eda de Pandas"},{"location":"apuntes09.html#series","text":"La clase de objetos Series son estructuras similares a los arrays de una dimensi\u00f3n. Son homog\u00e9neas, es decir, sus elementos tienen que ser del mismo tipo, y su tama\u00f1o es inmutable, es decir, no se puede cambiar el n\u00famero de elementos, aunque si su contenido. Creaci\u00f3n de un objeto de la clase Series (constructor) a partir de una lista o tupla Series (data=lista, index=indices, dtype=tipo) : Devuelve un objeto de tipo Series con los valores de lista, las claves especificadas en \u00edndices y el tipo de datos indicado en tipo. Si no se pasa la lista de \u00edndices se utilizan como \u00edndices los enteros del 0 al n-1, done n es el tama\u00f1o de la serie. Si no se pasa el tipo de dato se deduce. s = pd . Series ({ 'Matem\u00e1ticas' : 6.0 , 'Econom\u00eda' : 4.5 , 'Programaci\u00f3n' : 8.5 }) print ( s ) Matem\u00e1ticas 6.0 Econom\u00eda 4.5 Programaci\u00f3n 8.5 dtype: float64","title":"Series"},{"location":"apuntes09.html#atributos-de-la-clase-series","text":"s.size : Devuelve el n\u00famero de elementos de la serie s. Tambi\u00e9n podemos utilizar len(s). s.index : Devuelve una lista con los nombres de las filas del DataFrame. s.dtype : Devuelve el tipo de datos de los elementos de la serie s. s.values : Devuelve una lista con los valores asociados al \u00edndice. import pandas as pd s = pd . Series ([ 1 , 2 , 2 , 3 , 3 , 3 , 4 , 4 , 4 , 4 ]) print ( s . size ) print ( s . index ) print ( s . values ) print ( s . dtype )","title":"Atributos de la clase Series"},{"location":"apuntes09.html#acceso-a-elementos-de-una-serie","text":"s = pd . Series ({ 'Matem\u00e1ticas' : 6.0 , 'Econom\u00eda' : 4.5 , 'Programaci\u00f3n' : 8.5 }) print ( s [ 0 ]) # buscar por \u00edndice print ( s [ 1 ]) print () print ( s [ 1 : 3 ]) #el indice funciona como en listas print () print ( s [ 'Econom\u00eda' ]) #buscar por nombre print ( s . Econom\u00eda ) #otra forma de buscar por nombre cuando no tiene espacios print () print ( s [[ 'Programaci\u00f3n' , 'Matem\u00e1ticas' ]]) print () print ( s [ 'Matem\u00e1ticas' : 'Econom\u00eda' ]) #el \u00faltimo \u00edndice es v\u00e1lido print () print ( s . index ) print ( s . values )","title":"Acceso a elementos de una Serie"},{"location":"apuntes09.html#metodos-estadisticos-de-la-clase-series","text":"s.count() : Devuelve el n\u00famero de elementos que no son None ni NaN (no es un n\u00famero) en la serie s. s.sum() : Devuelve la suma de los datos de la serie s cuando los datos son de un tipo num\u00e9rico, o la concatenaci\u00f3n de ellos cuando son del tipo cadena str. s.cumsum() : Devuelve una serie con la suma acumulada de los datos de la serie s cuando los datos son de un tipo num\u00e9rico. s.value_counts() : Devuelve una serie con la frecuencia (n\u00famero de repeticiones) de cada valor de la serie s. s.min() : Devuelve el menor de los datos de la serie s. s.max() : Devuelve el mayor de los datos de la serie s. s.mean() : Devuelve la media de los datos de la serie s cuando los datos son de un tipo num\u00e9rico. s.std() : Devuelve la desviaci\u00f3n t\u00edpica de los datos de la serie s cuando los datos son de un tipo num\u00e9rico. s.describe() : Devuelve una serie con un resumen descriptivo que incluye el n\u00famero de datos, su suma, el m\u00ednimo, el m\u00e1ximo, la media, la desviaci\u00f3n t\u00edpica y los cuartiles. import pandas as pd s = pd . Series ([ 1 , 1 , 1 , 1 , 2 , 2 , 2 , 3 , 3 , 4 ]) print ( s . count ()) print ( s . sum ()) print () print ( s . cumsum ()) print () print ( s . value_counts ()) print ( s . value_counts ( normalize = True ) * 100 ) #tanto por 1 print ( s . min ()) print ( s . max ()) print ( s . mean ()) print ( s . std ()) print () print ( s . describe ())","title":"M\u00e9todos estad\u00edsticos de la clase Series"},{"location":"apuntes09.html#ordenar-una-serie","text":"s.sort_values (ascending=booleano): Devuelve la serie que resulta de ordenar los valores la serie s. Si argumento del par\u00e1metro ascending es True el orden es creciente y si es False decreciente. df.sort_index (ascending=booleano) : Devuelve la serie que resulta de ordenar el \u00edndice de la serie s. Si el argumento del par\u00e1metro ascending es True el orden es creciente y si es False decreciente. import pandas as pd s = pd . Series ({ 'Matem\u00e1ticas' : 6.0 , 'Econom\u00eda' : 4.5 , 'Programaci\u00f3n' : 8.5 }) print ( s . sort_values ()) print () print ( s . sort_index ( ascending = False ))","title":"Ordenar una Serie"},{"location":"apuntes09.html#eliminar-los-datos-desconocidos-en-una-serie","text":"Los datos desconocidos se representan en Pandas por NaN y los nulos por None. Tanto unos como otros suelen ser un problema a la hora de realizar algunos an\u00e1lisis de datos, por lo que es habitual eliminarlos. Para eliminarlos de una serie se utiliza el siguiente m\u00e9todo: s.dropna() : Elimina los datos desconocidos o nulos de la serie s. import pandas as pd import numpy as np s = pd . Series ([ 'a' , 'b' , None , 'c' , np . NaN , 'd' ]) print ( s ) print () print ( s . dropna ()) print () # la serie es inmutable print ( s )","title":"Eliminar los datos desconocidos en una Serie"},{"location":"apuntes09.html#dataframe","text":"Un objeto del tipo DataFrame define un conjunto de datos estructurado en forma de tabla donde cada columna es un objeto de tipo Series, es decir, todos los datos de una misma columna son del mismo tipo, y las filas son registros que pueden contender datos de distintos tipos. Un DataFrame contiene dos \u00edndices, uno para las filas y otro para las columnas, y se puede acceder a sus elementos mediante los nombres de las filas y las columnas. import pandas as pd #cada objeto del diccionario es una columna datos = { 'nombre' :[ 'Mar\u00eda' , 'Luis' , 'Carmen' , 'Antonio' ], 'edad' :[ 18 , 22 , 20 , 21 ], 'grado' :[ 'Econom\u00eda' , 'Medicina' , 'Arquitectura' , 'Econom\u00eda' ], 'correo' :[ 'maria@gmail.com' , 'luis@yahoo.es' , 'carmen@gmail.com' , 'antonio@gmail.com' ] } print ( type ( datos )) print () df = pd . DataFrame ( datos ) print ( type ( df )) print ( df ) df","title":"DataFrame"},{"location":"apuntes09.html#atributos-y-metodos-de-un-dataframe","text":"df.info() : Devuelve informaci\u00f3n (n\u00famero de filas, n\u00famero de columnas, \u00edndices, tipo de las columnas y memoria usado) sobre el DataFrame df. df.shape : Devuelve una tupla con el n\u00famero de filas y columnas del DataFrame df. df.size : Devuelve el n\u00famero de elementos del DataFrame. len(df) : Defuelve el n\u00famero de filas del DataFrame. df.columns : Devuelve una lista con los nombres de las columnas del DataFrame df. df.index : Devuelve una lista con los nombres de las filas del DataFrame df. df.dtypes : Devuelve una serie con los tipos de datos de las columnas del DataFrame df. df.head(n) : Devuelve las n primeras filas del DataFrame df. df.tail(n) : Devuelve las n \u00faltimas filas del DataFrame df. df.T : Devuelve la traspuesta, cambia filas por columnas.","title":"Atributos y m\u00e9todos de un DataFrame"},{"location":"apuntes09.html#renombrar-los-nombres-de-las-filas-y-las-columnas","text":"df.rename (columns=columnas, index=filas): Devuelve el DataFrame que resulta de renombrar las columnas indicadas en las claves del diccionario columnas con sus valores y las filas indicadas en las claves del diccionario filas con sus valores en el DataFrame df. df = pd . DataFrame () nombres = [ 'Juan' , 'Laura' , 'Pepe' ] edades = [ 42 , 40 , 37 ] df [ 'Nombre' ] = nombres df [ 'Edad' ] = edades df . rename ( columns = { 'Nombre' : 'Nom' }, index = { 0 : 100 }) print ( df )","title":"Renombrar los nombres de las filas y las columnas"},{"location":"apuntes09.html#acceso-a-los-elementos-de-un-dataframe","text":"Accesos mediante posiciones: df.iloc[i,j] : Devuelve el elemento que se encuentra en la fila i y la columna j del DataFrame df. Pueden indicarse secuencias de \u00edndices para obtener partes del DataFrame. df.iloc[filas, columnas] : Devuelve un DataFrame con los elementos de las filas de la lista filas y de las columnas de la lista columnas. df.iloc[i] :Devuelve una serie con los elementos de la fila i del DataFrame df. df = pd . DataFrame ([[ 1 , 2 ], [ 4 , 5 ], [ 7 , 8 ]], index = [ 'cobra' , 'viper' , 'sidewinder' ], columns = [ 'max_speed' , 'shield' ]) df df . iloc [ 0 ] df . iloc [[ 0 , 1 ]] df . iloc [: 3 ] Acceso mediante nombres: df.loc[fila, columna] : Devuelve el elemento que se encuentra en la fila con nombre fila y la columna de con nombre columna del DataFrame df. df . loc [ 'viper' ] df . loc [ 'cobra' , 'shield' ]","title":"Acceso a los elementos de un Dataframe"},{"location":"apuntes10.html","text":"Acceso a ficheros \u00b6 Drive \u00b6 Para leer ficheros de tu drive primero se debe montar la carpeta que quieres leer como unidad siguiendo los siguientes pasos: Importar la librer\u00eda drive para montar la unidad. from google.colab import drive Montar la carpeta a la que queremos acceder. drive . mount ( './drive' ) El directorio ra\u00edz de Colab es /content. Web \u00b6 # Podemos usar 'wget' desde l\u00ednea de comando ! wget https : // web . ua . es / secciones - ua / images / layout / logo - ua . jpg Fichero local \u00b6 from google.colab import files files . upload () Al ejecutarlo, nos aparecer\u00e1 el t\u00edpico bot\u00f3n de subida de archivos que abrir\u00e1 una ventana para seleccionar los archivos que queremos subir. Otra forma ser\u00eda desde el bot\u00f3n de subir de la siguiente imagen:","title":"4.2.1. ACCESO A FICHEROS"},{"location":"apuntes10.html#acceso-a-ficheros","text":"","title":"Acceso a ficheros"},{"location":"apuntes10.html#drive","text":"Para leer ficheros de tu drive primero se debe montar la carpeta que quieres leer como unidad siguiendo los siguientes pasos: Importar la librer\u00eda drive para montar la unidad. from google.colab import drive Montar la carpeta a la que queremos acceder. drive . mount ( './drive' ) El directorio ra\u00edz de Colab es /content.","title":"Drive"},{"location":"apuntes10.html#web","text":"# Podemos usar 'wget' desde l\u00ednea de comando ! wget https : // web . ua . es / secciones - ua / images / layout / logo - ua . jpg","title":"Web"},{"location":"apuntes10.html#fichero-local","text":"from google.colab import files files . upload () Al ejecutarlo, nos aparecer\u00e1 el t\u00edpico bot\u00f3n de subida de archivos que abrir\u00e1 una ventana para seleccionar los archivos que queremos subir. Otra forma ser\u00eda desde el bot\u00f3n de subir de la siguiente imagen:","title":"Fichero local"},{"location":"apuntes11.html","text":"Creaci\u00f3n de un DataFrame a partir de un fichero CSV o Excel \u00b6 Dependiendo del tipo de fichero, existen distintas funciones para importar un DataFrame desde un fichero. read_csv(fichero.csv, sep=separador, header=n, index_col=m, na_values=no-validos, decimal=separador-decimal, thousands=separador-miles) : Devuelve un objeto del tipo DataFrame con los datos del fichero CSV fichero.csv usando como separador de los datos la cadena separador. Como nombres de columnas se utiliza los valores de la lista n y como nombres de filas los valores de la lista m. Si no se indica m se utilizan como nombres de filas los enteros empezando en 0. Los valores inclu\u00eddos en la lista no-validos se convierten en NaN. Para los datos num\u00e9ricos se utiliza como separador de decimales el car\u00e1cter indicado en separador-decimal. read_excel(fichero.xlsx, sheet_name=hoja, header=n, index_col=m, na_values=no-validos, decimal=separador-decimal) : Devuelve un objeto del tipo DataFrame con los datos de la hoja de c\u00e1lculo hoja del fichero Excel fichero.xlsx. Como nombres de columnas se utiliza los valores de la lista n y como nombres de filas los valores de la lista m. Si no se indica m se utilizan como nombres de filas los enteros empezando en 0. Los valores inclu\u00eddos en la lista no-validos se convierten en NaN. Para los datos num\u00e9ricos se utiliza como separador de decimales el car\u00e1cter indicado en separador-decimal. Podemos leer otro tipo de ficheros, con las extensiones mas utilizadas, con las librer\u00edas: read_table read_json read_xml read_pickle read_sql_table.... Lo podemos leer directamente del directorio: import pandas as pd # Importaci\u00f3n del fichero datos.csv df = pd . read_csv ( 'datos.csv' , sep = ';' ) df . head () #muestra empezando por cabecera, df.tail() muestra los \u00faltimos Tambi\u00e9n lo podemos descargar directamente de una web: # Importaci\u00f3n del fichero datos/colesteroles.csv df = pd . read_csv ( 'https://raw.githubusercontent.com/asalber/manual-python/master/datos/colesteroles.csv' , sep = ';' , decimal = ',' ) df . head () Con diferente separaci\u00f3n: df = pd . read_csv ( 'salida.csv' , sep = '|' ) df . head () # Importaci\u00f3n del fichero ovnis.csv df = pd . read_csv ( 'ovnis.csv' ) tupla = df . shape #devuelve una tupla con el n\u00famero de filas y columnas print ( f \"N\u00ba filas = { tupla [ 0 ] } n\u00ba columnas = { tupla [ 1 ] } \" ) print ( len ( df )) df . head ( 200 ) Leer solo una parte del fichero (\u00fatil para ficheros grandes). df = pd . read_csv ( 'ovnis.csv' , nrows = 7 ) Leer ficheros xls df = pd . read_excel ( 'ejemplo3.xlsx' ) df . head () Leer ficheros xls en una hoja distinta de la principal: df = pd . read_excel ( 'ejemplo3.xlsx' , sheet_name = 'Hoja2' ) df . head () Leer excel sin cabecera: df = pd . read_excel ( \"ejemplo4.xlsx\" , sheet_name = \"Hoja3\" , header = None ) df Ficheros PDF con TIKA \u00b6 Apache Tika es una librer\u00eda que se utiliza para la detecci\u00f3n de diferentes tipos de documentos y la extracci\u00f3n de contenido de varios formatos de archivo. Se instala la librer\u00eda Tika con el siguiente comando: pip install tika from tika import parser Para extraer el contenido del archivo PDF parsed_pdf = parser . from_file ( \"sample.pdf\" ) data = parsed_pdf [ 'content' ] print ( data ) print ( type ( data )) Exportaci\u00f3n de fichero \u00b6 Podemos exportar un Dataframe a un fichero con diferentes formatos: df.to_csv(fichero.csv, sep=separador, columns=booleano, index=booleano) : Exporta el DataFrame df al fichero fichero.csv en formato CSV usando como separador de los datos la cadena separador. Si se pasa True al par\u00e1metro columns se exporta tambi\u00e9n la fila con los nombres de columnas y si se pasa True al par\u00e1metro index se exporta tambi\u00e9n la columna con los nombres de las filas. df.to_excel(fichero.xlsx, sheet_name = hoja, columns=booleano, index=booleano) : Exporta el DataFrame df a la hoja de c\u00e1lculo hoja del fichero fichero.xlsx en formato Excel. Si se pasa True al par\u00e1metro columns se exporta tambi\u00e9n la fila con los nombres de columnas y si se pasa True al par\u00e1metro index se exporta tambi\u00e9n la columna con los nombres de las filas.","title":"4.2.2. CREAR DATAFRAMES"},{"location":"apuntes11.html#creacion-de-un-dataframe-a-partir-de-un-fichero-csv-o-excel","text":"Dependiendo del tipo de fichero, existen distintas funciones para importar un DataFrame desde un fichero. read_csv(fichero.csv, sep=separador, header=n, index_col=m, na_values=no-validos, decimal=separador-decimal, thousands=separador-miles) : Devuelve un objeto del tipo DataFrame con los datos del fichero CSV fichero.csv usando como separador de los datos la cadena separador. Como nombres de columnas se utiliza los valores de la lista n y como nombres de filas los valores de la lista m. Si no se indica m se utilizan como nombres de filas los enteros empezando en 0. Los valores inclu\u00eddos en la lista no-validos se convierten en NaN. Para los datos num\u00e9ricos se utiliza como separador de decimales el car\u00e1cter indicado en separador-decimal. read_excel(fichero.xlsx, sheet_name=hoja, header=n, index_col=m, na_values=no-validos, decimal=separador-decimal) : Devuelve un objeto del tipo DataFrame con los datos de la hoja de c\u00e1lculo hoja del fichero Excel fichero.xlsx. Como nombres de columnas se utiliza los valores de la lista n y como nombres de filas los valores de la lista m. Si no se indica m se utilizan como nombres de filas los enteros empezando en 0. Los valores inclu\u00eddos en la lista no-validos se convierten en NaN. Para los datos num\u00e9ricos se utiliza como separador de decimales el car\u00e1cter indicado en separador-decimal. Podemos leer otro tipo de ficheros, con las extensiones mas utilizadas, con las librer\u00edas: read_table read_json read_xml read_pickle read_sql_table.... Lo podemos leer directamente del directorio: import pandas as pd # Importaci\u00f3n del fichero datos.csv df = pd . read_csv ( 'datos.csv' , sep = ';' ) df . head () #muestra empezando por cabecera, df.tail() muestra los \u00faltimos Tambi\u00e9n lo podemos descargar directamente de una web: # Importaci\u00f3n del fichero datos/colesteroles.csv df = pd . read_csv ( 'https://raw.githubusercontent.com/asalber/manual-python/master/datos/colesteroles.csv' , sep = ';' , decimal = ',' ) df . head () Con diferente separaci\u00f3n: df = pd . read_csv ( 'salida.csv' , sep = '|' ) df . head () # Importaci\u00f3n del fichero ovnis.csv df = pd . read_csv ( 'ovnis.csv' ) tupla = df . shape #devuelve una tupla con el n\u00famero de filas y columnas print ( f \"N\u00ba filas = { tupla [ 0 ] } n\u00ba columnas = { tupla [ 1 ] } \" ) print ( len ( df )) df . head ( 200 ) Leer solo una parte del fichero (\u00fatil para ficheros grandes). df = pd . read_csv ( 'ovnis.csv' , nrows = 7 ) Leer ficheros xls df = pd . read_excel ( 'ejemplo3.xlsx' ) df . head () Leer ficheros xls en una hoja distinta de la principal: df = pd . read_excel ( 'ejemplo3.xlsx' , sheet_name = 'Hoja2' ) df . head () Leer excel sin cabecera: df = pd . read_excel ( \"ejemplo4.xlsx\" , sheet_name = \"Hoja3\" , header = None ) df","title":"Creaci\u00f3n de un DataFrame a partir de un fichero CSV o Excel"},{"location":"apuntes11.html#ficheros-pdf-con-tika","text":"Apache Tika es una librer\u00eda que se utiliza para la detecci\u00f3n de diferentes tipos de documentos y la extracci\u00f3n de contenido de varios formatos de archivo. Se instala la librer\u00eda Tika con el siguiente comando: pip install tika from tika import parser Para extraer el contenido del archivo PDF parsed_pdf = parser . from_file ( \"sample.pdf\" ) data = parsed_pdf [ 'content' ] print ( data ) print ( type ( data ))","title":"Ficheros PDF con TIKA"},{"location":"apuntes11.html#exportacion-de-fichero","text":"Podemos exportar un Dataframe a un fichero con diferentes formatos: df.to_csv(fichero.csv, sep=separador, columns=booleano, index=booleano) : Exporta el DataFrame df al fichero fichero.csv en formato CSV usando como separador de los datos la cadena separador. Si se pasa True al par\u00e1metro columns se exporta tambi\u00e9n la fila con los nombres de columnas y si se pasa True al par\u00e1metro index se exporta tambi\u00e9n la columna con los nombres de las filas. df.to_excel(fichero.xlsx, sheet_name = hoja, columns=booleano, index=booleano) : Exporta el DataFrame df a la hoja de c\u00e1lculo hoja del fichero fichero.xlsx en formato Excel. Si se pasa True al par\u00e1metro columns se exporta tambi\u00e9n la fila con los nombres de columnas y si se pasa True al par\u00e1metro index se exporta tambi\u00e9n la columna con los nombres de las filas.","title":"Exportaci\u00f3n de fichero"},{"location":"apuntes12.html","text":"Otras operaciones \u00b6 Eliminar columnas de un Dataframe \u00b6 Para eliminar columnas de un DataFrame se utilizan los siguientes m\u00e9todos: del d[nombre] : Elimina la columna con nombre nombre del DataFrame df. df.pop(nombre) : Elimina la columna con nombre nombre del DataFrame df y la devuelve como una serie. df = pd . read_csv ( 'https://raw.githubusercontent.com/asalber/manual-python/master/datos/colesterol.csv' ) edad = df . pop ( 'edad' ) print ( df ) Reemplazar valores vac\u00edos \u00b6 Si existen valores vac\u00edos y no se quieren eliminar, podemos rellenar las celdas vac\u00edas con un valor: df . fillna ( 40 , inplace = True ) Se han rellenado todos los campos que aparec\u00edan vac\u00edos con el valor 40. Podemos comprobar que ya no existen campos vac\u00edos con df.info(). Reemplazar s\u00f3lo las columnas especificadas \u00b6 df = pd . read_csv ( 'https://raw.githubusercontent.com/asalber/manual-python/master/datos/colesterol.csv' ) df [ \"colesterol\" ] . fillna ( 130 , inplace = True ) Reemplazar usando Media, Mediana o Moda \u00b6 df = pd . read_csv ( 'https://raw.githubusercontent.com/asalber/manual-python/master/datos/colesterol.csv' ) x = df [ \"colesterol\" ] . mean () df [ \"colesterol\" ] . fillna ( x , inplace = True ) Eliminaci\u00f3n de duplicados \u00b6 df = pd . read_excel ( 'ejemplo4.xlsx' , sheet_name = \"Hoja3\" , header = None ) df . duplicated () df . drop_duplicates ( inplace = True ) Convertir una columna al tipo datetime \u00b6 Puede ocurrir que una columna que contiene datos de tipo cadena, realmente representa fechas. Para convertir esta columna al formato fecha se utiliza el siguiente m\u00e9todo: to_datetime(columna, formato) : Devuelve la serie que resulta de convertir las cadenas de la columna con el nombre columna en fechas del tipo datetime con el formado especificado en formato. df = pd . DataFrame ({ 'Name' : [ 'Mar\u00eda' , 'Carlos' , 'Carmen' ], 'Nacimiento' :[ '05-03-2000' , '20-05-2001' , '10-12-1999' ]}) print ( pd . to_datetime ( df . Nacimiento , format = ' %d -%m-%Y' )) Agrupaci\u00f3n de un DataFrame \u00b6 Muchas veces necesitamos agrupar los datos de un DataFrame de acuerdo a los valores de una o varias columnas o categor\u00eda, como por ejemplo el sexo o el pa\u00eds. df.groupby(columnas).get_group(valores) : Devuelve un DataFrame con las filas del DataFrame df que cumplen que las columnas de la lista columnas presentan los valores de la tupla valores. La lista columnas y la tupla valores deben tener el mismo tama\u00f1o. df = pd . read_csv ( 'https://raw.githubusercontent.com/asalber/manual-python/master/datos/colesterol.csv' ) print ( df . groupby ( 'sexo' ) . get_group ( 'M' )) Agregaci\u00f3n por grupos \u00b6 df.groupby(columnas).agg(funciones) : Devuelve un DataFrame con el resultado de aplicar las funciones de agregaci\u00f3n de la lista funciones a cada uno de los DataFrames que resultan de dividir el DataFrame seg\u00fan las columnas de la lista columnas. Una funci\u00f3n de agregaci\u00f3n toma como argumento una lista y devuelve una \u00fanico valor. Algunas de las funciones de agregaci\u00f3n m\u00e1s comunes son: np.min : Devuelve el m\u00ednimo de una lista de valores. np.max : Devuelve el m\u00e1ximo de una lista de valores. np.count_nonzero : Devuelve el n\u00famero de valores no nulos de una lista de valores. np.sum : Devuelve la suma de una lista de valores. np.mean : Devuelve la media de una lista de valores. np.std : Devuelve la desviaci\u00f3n t\u00edpica de una lista de valores. import numpy as np df = pd . read_csv ( 'https://raw.githubusercontent.com/asalber/manual-python/master/datos/colesterol.csv' ) print ( df . groupby ( 'sexo' ) . agg ( np . mean ))","title":"4.2.3. OTRAS OPERACIONES"},{"location":"apuntes12.html#otras-operaciones","text":"","title":"Otras operaciones"},{"location":"apuntes12.html#eliminar-columnas-de-un-dataframe","text":"Para eliminar columnas de un DataFrame se utilizan los siguientes m\u00e9todos: del d[nombre] : Elimina la columna con nombre nombre del DataFrame df. df.pop(nombre) : Elimina la columna con nombre nombre del DataFrame df y la devuelve como una serie. df = pd . read_csv ( 'https://raw.githubusercontent.com/asalber/manual-python/master/datos/colesterol.csv' ) edad = df . pop ( 'edad' ) print ( df )","title":"Eliminar columnas de un Dataframe"},{"location":"apuntes12.html#reemplazar-valores-vacios","text":"Si existen valores vac\u00edos y no se quieren eliminar, podemos rellenar las celdas vac\u00edas con un valor: df . fillna ( 40 , inplace = True ) Se han rellenado todos los campos que aparec\u00edan vac\u00edos con el valor 40. Podemos comprobar que ya no existen campos vac\u00edos con df.info().","title":"Reemplazar valores vac\u00edos"},{"location":"apuntes12.html#reemplazar-solo-las-columnas-especificadas","text":"df = pd . read_csv ( 'https://raw.githubusercontent.com/asalber/manual-python/master/datos/colesterol.csv' ) df [ \"colesterol\" ] . fillna ( 130 , inplace = True )","title":"Reemplazar s\u00f3lo las columnas especificadas"},{"location":"apuntes12.html#reemplazar-usando-media-mediana-o-moda","text":"df = pd . read_csv ( 'https://raw.githubusercontent.com/asalber/manual-python/master/datos/colesterol.csv' ) x = df [ \"colesterol\" ] . mean () df [ \"colesterol\" ] . fillna ( x , inplace = True )","title":"Reemplazar usando Media, Mediana o Moda"},{"location":"apuntes12.html#eliminacion-de-duplicados","text":"df = pd . read_excel ( 'ejemplo4.xlsx' , sheet_name = \"Hoja3\" , header = None ) df . duplicated () df . drop_duplicates ( inplace = True )","title":"Eliminaci\u00f3n de duplicados"},{"location":"apuntes12.html#convertir-una-columna-al-tipo-datetime","text":"Puede ocurrir que una columna que contiene datos de tipo cadena, realmente representa fechas. Para convertir esta columna al formato fecha se utiliza el siguiente m\u00e9todo: to_datetime(columna, formato) : Devuelve la serie que resulta de convertir las cadenas de la columna con el nombre columna en fechas del tipo datetime con el formado especificado en formato. df = pd . DataFrame ({ 'Name' : [ 'Mar\u00eda' , 'Carlos' , 'Carmen' ], 'Nacimiento' :[ '05-03-2000' , '20-05-2001' , '10-12-1999' ]}) print ( pd . to_datetime ( df . Nacimiento , format = ' %d -%m-%Y' ))","title":"Convertir una columna al tipo datetime"},{"location":"apuntes12.html#agrupacion-de-un-dataframe","text":"Muchas veces necesitamos agrupar los datos de un DataFrame de acuerdo a los valores de una o varias columnas o categor\u00eda, como por ejemplo el sexo o el pa\u00eds. df.groupby(columnas).get_group(valores) : Devuelve un DataFrame con las filas del DataFrame df que cumplen que las columnas de la lista columnas presentan los valores de la tupla valores. La lista columnas y la tupla valores deben tener el mismo tama\u00f1o. df = pd . read_csv ( 'https://raw.githubusercontent.com/asalber/manual-python/master/datos/colesterol.csv' ) print ( df . groupby ( 'sexo' ) . get_group ( 'M' ))","title":"Agrupaci\u00f3n de un DataFrame"},{"location":"apuntes12.html#agregacion-por-grupos","text":"df.groupby(columnas).agg(funciones) : Devuelve un DataFrame con el resultado de aplicar las funciones de agregaci\u00f3n de la lista funciones a cada uno de los DataFrames que resultan de dividir el DataFrame seg\u00fan las columnas de la lista columnas. Una funci\u00f3n de agregaci\u00f3n toma como argumento una lista y devuelve una \u00fanico valor. Algunas de las funciones de agregaci\u00f3n m\u00e1s comunes son: np.min : Devuelve el m\u00ednimo de una lista de valores. np.max : Devuelve el m\u00e1ximo de una lista de valores. np.count_nonzero : Devuelve el n\u00famero de valores no nulos de una lista de valores. np.sum : Devuelve la suma de una lista de valores. np.mean : Devuelve la media de una lista de valores. np.std : Devuelve la desviaci\u00f3n t\u00edpica de una lista de valores. import numpy as np df = pd . read_csv ( 'https://raw.githubusercontent.com/asalber/manual-python/master/datos/colesterol.csv' ) print ( df . groupby ( 'sexo' ) . agg ( np . mean ))","title":"Agregaci\u00f3n por grupos"},{"location":"apuntes13.html","text":"Limpieza de datos \u00b6 La limpieza de datos (data cleaning o data scrubbing) consiste en el descubrimiento y correcci\u00f3n o eliminaci\u00f3n de datos err\u00f3neos en una tabla, base de datos o dataframe. El proceso de limpieza de datos permite identificar datos incompletos, incorrectos, inexactos, no pertinentes, etc y luego substituir, modificar o eliminar estos datos sucios (\"data duty\") . Los datos de una fuente, a la hora de consumirlos suelen ser: Inconsistentes : con informaci\u00f3n relevante y no relevante. Imprecisos : contienen datos incorrectos, valores perdidos, etc. Repetitivos : duplicidad de datos. Exploraci\u00f3n \u00b6 Una vez que tenemos descargado el dataset, lo primero que hay que hacer es reconocer con qu\u00e9 nos estamos enfrentando. Para ello visualizamos el dataset. df = pd . read_csv ( 'https://raw.githubusercontent.com/asalber/manual-python/master/datos/colesterol.csv' ) df . head () Tambi\u00e9n podemos visualizar desde abajo con tail. Para obtener datos r\u00e1pidos de lo que hay dentro, utilizamos el m\u00e9todo describe. df . describe () La funci\u00f3n describe nos muestra los datos relevantes para columnas que contengan n\u00fameros. Para obtener datos de columnas que contienen valores de otro tipo como por ejemplo cadenas, debemos agregar un par\u00e1metro para que tome en cuenta a todo el dataset y no solo a los valores num\u00e9ricos. df . describe ( include = 'all' ) Las tres categor\u00edas que se incluyen son: unique : cantidad de valores \u00fanicos. top : valor \u00fanico con m\u00e1s repeticiones. freq : cantidad de veces que aparece el valor m\u00e1s repetido. Eliminar las columnas irrelevantes \u00b6 Las columnas que no sean interesantes para el desarrollo de nuestro problema las podemos eliminar. En nuestro ejemplo vamos a eliminar la columna nombre pero como luego nos va a hacer falta para continuar con los ejemplos, vamos a asignar el dataframe a df1 y sobre \u00e9ste eliminamos el campo nombre. df1 = df df1 . drop ( 'nombre' , inplace = True , axis = 1 ) Tipos de datos \u00b6 Obtener los tipos de datos de las diferentes columnas para ver si son correctos. df . dtypes Si comprobamos que hay alg\u00fan tipo de dato err\u00f3neo podemos cambiarlo, por ejemplo el campo edad que es entero queremos cambiarlo a float: df [[ 'edad' ]] = df [[ 'edad' ]] . astype ( float ) df . dtypes Tratar con los datos perdidos \u00b6 Otro paso en la limpieza de datos es averiguar si el dataset est\u00e1 completo o tiene datos perdidos. missing_data = df . isnull () for column in missing_data . columns . values . tolist (): print ( column ) print ( missing_data [ column ] . value_counts ()) print ( '' ) Los valores perdidos los podemos reemplazar, para ello hay que averiguar qu\u00e9 tipo de dato es y c\u00f3mo tratarlo. Strings : la mejor pr\u00e1ctica es reemplazar el dato perdido por el que se repite con m\u00e1s frecuencia. Integers : el dato perdido debe reemplazarse por el promedio de su misma columna. En el caso de que sea strings : Primero comprobamos el valor que m\u00e1s se repite que vendr\u00e1 indicado en la etiqueta top . import numpy as np df . describe ( include = [ np . object ]) En el caso del ejemplo, como no hay ning\u00fan valor vac\u00edo en el campo nombre, vamos a eliminar el nombre de Rosa D\u00edaz D\u00edaz para luego poder reemplazarlo: df . replace ( 'Rosa D\u00edaz D\u00edaz' , np . nan , inplace = True ) Luego lo reemplazamos: df [ 'nombre' ] . replace ( np . nan , 'Jos\u00e9 Luis Martinez Izquierdo' , inplace = True En el caso de que sea integer : Se obtiene la media: media = df [ 'colesterol' ] . astype ( float ) . mean ( axis = 0 ) print ( media ) Reemplazamos el valor: df [ 'colesterol' ] . replace ( np . nan , media , inplace = True ) Reemplazar usando Media, Mediana o Moda \u00b6 df = pd . read_csv ( 'https://raw.githubusercontent.com/asalber/manual-python/master/datos/colesterol.csv' ) x = df [ \"colesterol\" ] . mean () df [ \"colesterol\" ] . fillna ( x , inplace = True ) Reemplazar el valor de las filas si es necesario \u00b6 Para aplicar mecanismos de inteligencia artificial, necesitamos que todos los datos sean num\u00e9ricos y nos podr\u00eda interesar modificar los valores de alg\u00fan campo a num\u00e9rico. En el caso del ejemplo, queremos cambiar el campo sexo y asignarle un 1 si es hombre y un 0 si es mujer. replace = { 'H' : 1 , 'M' : 0 } df = df . replace ({ \"sexo\" : replace }) Outliers \u00b6 Outlier es cualquier dato que difiere significativamente del promedio o rango observado del resto de nuestros datos. Estos valores se pueden detectar a simple vista revisando todos los datos de la columna o tambi\u00e9n mediante gr\u00e1ficos. Datos categ\u00f3ricos \u00b6 Los datos categ\u00f3ricos toman un n\u00famero limitado y generalmente fijo de valores posibles. Por ejemplo: g\u00e9nero, clase social, tipo de sangre, etc. Existen dos tipos de variables categ\u00f3ricas: Variables ordinales : sus valores pueden ser ordenados jer\u00e1rquicamente, como, por ejemplo, el nivel de educaci\u00f3n de una persona (sin estudios, primaria, secundaria o superiores). Variables nominales : no se puede establecer un orden en sus categor\u00edas. Algunos ejemplos son el sexo de una persona (hombre o mujer), el color del pelo (casta\u00f1o, rubio, negro o rojo) o el pa\u00eds de nacimiento (Espa\u00f1a, Italia, Francia, etc.). Realiza los siguientes ejercicios: Limpieza y preparaci\u00f3n I Limpieza y preparaci\u00f3n II","title":"5. LIMPIEZA DE DATOS"},{"location":"apuntes13.html#limpieza-de-datos","text":"La limpieza de datos (data cleaning o data scrubbing) consiste en el descubrimiento y correcci\u00f3n o eliminaci\u00f3n de datos err\u00f3neos en una tabla, base de datos o dataframe. El proceso de limpieza de datos permite identificar datos incompletos, incorrectos, inexactos, no pertinentes, etc y luego substituir, modificar o eliminar estos datos sucios (\"data duty\") . Los datos de una fuente, a la hora de consumirlos suelen ser: Inconsistentes : con informaci\u00f3n relevante y no relevante. Imprecisos : contienen datos incorrectos, valores perdidos, etc. Repetitivos : duplicidad de datos.","title":"Limpieza de datos"},{"location":"apuntes13.html#exploracion","text":"Una vez que tenemos descargado el dataset, lo primero que hay que hacer es reconocer con qu\u00e9 nos estamos enfrentando. Para ello visualizamos el dataset. df = pd . read_csv ( 'https://raw.githubusercontent.com/asalber/manual-python/master/datos/colesterol.csv' ) df . head () Tambi\u00e9n podemos visualizar desde abajo con tail. Para obtener datos r\u00e1pidos de lo que hay dentro, utilizamos el m\u00e9todo describe. df . describe () La funci\u00f3n describe nos muestra los datos relevantes para columnas que contengan n\u00fameros. Para obtener datos de columnas que contienen valores de otro tipo como por ejemplo cadenas, debemos agregar un par\u00e1metro para que tome en cuenta a todo el dataset y no solo a los valores num\u00e9ricos. df . describe ( include = 'all' ) Las tres categor\u00edas que se incluyen son: unique : cantidad de valores \u00fanicos. top : valor \u00fanico con m\u00e1s repeticiones. freq : cantidad de veces que aparece el valor m\u00e1s repetido.","title":"Exploraci\u00f3n"},{"location":"apuntes13.html#eliminar-las-columnas-irrelevantes","text":"Las columnas que no sean interesantes para el desarrollo de nuestro problema las podemos eliminar. En nuestro ejemplo vamos a eliminar la columna nombre pero como luego nos va a hacer falta para continuar con los ejemplos, vamos a asignar el dataframe a df1 y sobre \u00e9ste eliminamos el campo nombre. df1 = df df1 . drop ( 'nombre' , inplace = True , axis = 1 )","title":"Eliminar las columnas irrelevantes"},{"location":"apuntes13.html#tipos-de-datos","text":"Obtener los tipos de datos de las diferentes columnas para ver si son correctos. df . dtypes Si comprobamos que hay alg\u00fan tipo de dato err\u00f3neo podemos cambiarlo, por ejemplo el campo edad que es entero queremos cambiarlo a float: df [[ 'edad' ]] = df [[ 'edad' ]] . astype ( float ) df . dtypes","title":"Tipos de datos"},{"location":"apuntes13.html#tratar-con-los-datos-perdidos","text":"Otro paso en la limpieza de datos es averiguar si el dataset est\u00e1 completo o tiene datos perdidos. missing_data = df . isnull () for column in missing_data . columns . values . tolist (): print ( column ) print ( missing_data [ column ] . value_counts ()) print ( '' ) Los valores perdidos los podemos reemplazar, para ello hay que averiguar qu\u00e9 tipo de dato es y c\u00f3mo tratarlo. Strings : la mejor pr\u00e1ctica es reemplazar el dato perdido por el que se repite con m\u00e1s frecuencia. Integers : el dato perdido debe reemplazarse por el promedio de su misma columna. En el caso de que sea strings : Primero comprobamos el valor que m\u00e1s se repite que vendr\u00e1 indicado en la etiqueta top . import numpy as np df . describe ( include = [ np . object ]) En el caso del ejemplo, como no hay ning\u00fan valor vac\u00edo en el campo nombre, vamos a eliminar el nombre de Rosa D\u00edaz D\u00edaz para luego poder reemplazarlo: df . replace ( 'Rosa D\u00edaz D\u00edaz' , np . nan , inplace = True ) Luego lo reemplazamos: df [ 'nombre' ] . replace ( np . nan , 'Jos\u00e9 Luis Martinez Izquierdo' , inplace = True En el caso de que sea integer : Se obtiene la media: media = df [ 'colesterol' ] . astype ( float ) . mean ( axis = 0 ) print ( media ) Reemplazamos el valor: df [ 'colesterol' ] . replace ( np . nan , media , inplace = True )","title":"Tratar con los datos perdidos"},{"location":"apuntes13.html#reemplazar-usando-media-mediana-o-moda","text":"df = pd . read_csv ( 'https://raw.githubusercontent.com/asalber/manual-python/master/datos/colesterol.csv' ) x = df [ \"colesterol\" ] . mean () df [ \"colesterol\" ] . fillna ( x , inplace = True )","title":"Reemplazar usando Media, Mediana o Moda"},{"location":"apuntes13.html#reemplazar-el-valor-de-las-filas-si-es-necesario","text":"Para aplicar mecanismos de inteligencia artificial, necesitamos que todos los datos sean num\u00e9ricos y nos podr\u00eda interesar modificar los valores de alg\u00fan campo a num\u00e9rico. En el caso del ejemplo, queremos cambiar el campo sexo y asignarle un 1 si es hombre y un 0 si es mujer. replace = { 'H' : 1 , 'M' : 0 } df = df . replace ({ \"sexo\" : replace })","title":"Reemplazar el valor de las filas si es necesario"},{"location":"apuntes13.html#outliers","text":"Outlier es cualquier dato que difiere significativamente del promedio o rango observado del resto de nuestros datos. Estos valores se pueden detectar a simple vista revisando todos los datos de la columna o tambi\u00e9n mediante gr\u00e1ficos.","title":"Outliers"},{"location":"apuntes13.html#datos-categoricos","text":"Los datos categ\u00f3ricos toman un n\u00famero limitado y generalmente fijo de valores posibles. Por ejemplo: g\u00e9nero, clase social, tipo de sangre, etc. Existen dos tipos de variables categ\u00f3ricas: Variables ordinales : sus valores pueden ser ordenados jer\u00e1rquicamente, como, por ejemplo, el nivel de educaci\u00f3n de una persona (sin estudios, primaria, secundaria o superiores). Variables nominales : no se puede establecer un orden en sus categor\u00edas. Algunos ejemplos son el sexo de una persona (hombre o mujer), el color del pelo (casta\u00f1o, rubio, negro o rojo) o el pa\u00eds de nacimiento (Espa\u00f1a, Italia, Francia, etc.). Realiza los siguientes ejercicios: Limpieza y preparaci\u00f3n I Limpieza y preparaci\u00f3n II","title":"Datos categ\u00f3ricos"},{"location":"apuntes14.html","text":"Data Warehouse \u00b6 Es una gran base de datos, normalmente medida en gigabytes o terabytes. Es un proceso para recopilar y administrar datos de diversas fuentes para proporcionar informaci\u00f3n empresarial significativa (data warehousing). Conecta y analiza datos comerciales de fuentes heterog\u00e9neas. Es el n\u00facleo del sistema de Business Intelligence que est\u00e1 dise\u00f1ado para el an\u00e1lisis y generaci\u00f3n de informes de datos. Es un proceso de transformar datos en informaci\u00f3n y ponerlos a disposici\u00f3n de los usuarios. Contiene datos de calidad como datos de clientes, empleados o ventas. Caracter\u00edsticas de los Data Warehouse \u00b6 Comparaci\u00f3n modelo transaccional y anal\u00edtico \u00b6 Data Mart \u00b6 Seg\u00fan Kimball, \u201cUn Data Mart es un conjunto de datos flexible, idealmente basado en el nivel de granularidad mayor que sea posible, presentado en un modelo dimensional que es capaz de comportarse bien ante cualquier consulta del usuario. En su definici\u00f3n m\u00e1s sencilla, un data mart representa un \u00fanico proceso de negocio.\u201d Un proceso de negocio es un conjunto de tareas relacionadas l\u00f3gicamente, llevadas a cabo para lograr un resultado de negocio definido. Data Mart VS Data Warehouse \u00b6 La \u00fanica diferencia es en cuanto al alcance. Un Data Warehouse es un sistema centralizado con datos globales de la empresa y de todos sus procesos operacionales mientras que un Data Mart es un subconjunto tem\u00e1tico de datos, orientado a un proceso o un \u00e1rea de negocio espec\u00edfica. Por ejemplo, a pedidos de clientes, a compras, a inventario de almac\u00e9n, etc. La existencia de un Data Warehouse no descarta la existencia de Data Marts ni viceversa. Es decir, puede haber organizaciones que tengan s\u00f3lo un Data Warehouse, que s\u00f3lo tengan Data Marts, o que tengan un Data Warehouse y Data Marts. ETL \u00b6 En muchas ocasiones, la informaci\u00f3n no pasa directamente de las fuentes al Data Mart o Data Warehouse, sino que lo hace a trav\u00e9s de unas bases de datos intermedias, que son necesarias dada la complejidad y disparidad de las fuentes. Los datos, antes de entrar en el Data Mart o Data Warehouse, se almacenan en un \u00e1rea de staging y/o un ODS (Operational Data Store). Un \u00e1rea de staging es un \u00e1rea temporal que se encuentra en el flujo de datos entre las fuentes y el Data Mart o Data Warehouse con el fin de facilitar la extracci\u00f3n de datos, de realizar tareas de limpieza (data cleasing) o de mejorar la calidad de los datos. Un ODS es un \u00e1rea que va a dar soporte a los sistemas transaccionales, desde los que se alimenta con una periodicidad muy baja, y sirve como base de datos de consulta, a la que se conectarn herramientas de reporting con el fin de que el sistema transaccional tenga una menor carga de trabajo. Metodolog\u00edas \u00b6 Existen muchas metodolog\u00edas de dise\u00f1o y construcci\u00f3n de un Data Warehouse. Cada fabricante de software de inteligencia de negocios busca imponer una metodolog\u00eda con sus productos. Sin embargo, se imponen entre la mayor\u00eda dos metodolog\u00edas, la de Kimball y la de Inmon. Metodolog\u00eda Kimball : Ralph Kimball sigue una metodolog\u00eda bottom-up y una estructura desnormalizada. Se define el dise\u00f1o en detalle de partes individuales para luego enlazarlas entre ellas para formar un sistema completo. Es decir, con la metodolog\u00eda de Kimball se van dise\u00f1ando a medida, seg\u00fan las necesidades de los departamentos, los diferentes Data Marts, para unirlos y formar el Data Warehouse de la empresa. Metodolog\u00eda Inmon :se caracteriza por tener un repositorio central de datos \u00fanico. Se basa en la metodolog\u00eda top-down, que es aquella que toma decisiones partiendo de las variables m\u00e1s globales para ir descendiendo hasta las m\u00e1s espec\u00edficas. Es decir, Inmon parte de un almac\u00e9n de datos global del que se ir\u00e1n nutriendo los diferentes Data Marts, evitando as\u00ed las diferentes inconsistencias que puedan existir entre diferentes departamentos. Alternativa: Data Vault : es una t\u00e9cnica h\u00edbrida entre el modelo normalizado y el modelo dimensional donde se toman lo mejor de cada una y se solucionan sus defectos. La definici\u00f3n que da Dan Listedt es la siguiente: \u201cData Vault es un conjunto de tablas normalizadas orientadas al detalle, con seguimiento hist\u00f3rico y vinculadas de forma \u00fanica, que dan soporte a una o m\u00e1s \u00e1reas funcionales del negocio. Se trata de un enfoque h\u00edbrido que engloba lo mejor de la tercera forma normal (3NF) y el esquema en estrella. El dise\u00f1o es flexible, escalable, consistente y adaptable a las necesidades de la empresa. Se trata de un modelo de datos dise\u00f1ado espec\u00edficamente para satisfacer las necesidades de los almacenes de datos empresariales actuales.\u201d Comparativa entre Inmon y Kimball: Modelado \u00b6 Esquema en estrella (star schema) : hay una \u00fanica tabla central, la tabla de hechos, que contiene todas las medidas y una tabla adicional por cada una de las perspectivas desde las que queremos analizar dicha informaci\u00f3n, es decir por cada una de las dimensiones. Esquema en copo de nieve (snowflake schema) . La diferencia es que algunas dimensiones no est\u00e1n relacionadas directamente con la tabla de hechos, sino que se relacionan con ella a trav\u00e9s de otras dimensiones. Tambi\u00e9n tenemos una tabla de hechos, situada en el centro, que contiene todas las medidas y una o varias tablas adicionales, con un mayor nivel de normalizaci\u00f3n. Esquema h\u00edbrido . Tiene parte en estrella y parte en copo de nieve. Modelado Dimensional \u00b6 Utilizado en la mayor\u00eda de las soluciones de BI. Es una mezcla de normalizaci\u00f3n y desnormalizaci\u00f3n, llamada Normalizaci\u00f3n Dimensional. Se utiliza tanto en el dise\u00f1o de Data Marts como de Data Warehouses. Hay dos tipos de tablas: Tablas de Dimensi\u00f3n (Dimension Tables) Tablas de Hechos (Fact Tables) Tablas de Hechos \u00b6 Son tablas que representan detalles del proceso de negocio a analizar, por ejemplo, las ventas, las compras, las incidencias recibidas, los pagos, los apuntes contables, los clics sobre nuestro sitio web, etc. Contienen datos num\u00e9ricos y medidas (m\u00e9tricas). Contienen tambi\u00e9n elementos (claves externas) para contextualizar dichas medidas, como por ejemplo el producto, la fecha, el cliente, la cuenta contable, etc. Elementos que las componen: Clave principal : identifica de forma \u00fanica cada fila. Claves externas : apuntan hacia las claves principales de cada una de las dimensiones que tienen relaci\u00f3n con dicha tabla de hechos. Medidas : representan columnas que contienen datos cuantificables, num\u00e9ricos, que se pueden agregar como por ejemplo, cantidad, importe, precio, margen, etc. Metadatos y linaje: permite obtener informaci\u00f3n adicional sobre la fila, por ejemplo, que d\u00eda se incorpor\u00f3 al Data Warehouse, de qu\u00e9 origen proviene, etc. Tablas de Dimensiones \u00b6 Las tablas de dimensiones almacenan una serie de atributos o caracter\u00edsticas, por las cuales podemos agrupar, rebanar o filtrar informaci\u00f3n. Nos permiten contextualizar los hechos, agregando diferentes perspectivas de an\u00e1lisis. Algunas veces los atributos est\u00e1n organizados en jerarqu\u00edas que permiten analizar los datos de forma agrupada. Por ejemplo, en una dimensi\u00f3n Producto podemos encontrar una jerarqu\u00eda formada por los atributos Categor\u00eda, Subcategor\u00eda y Producto. Tienen una clave principal diferente que se conoce con el nombre de clave subrogada. Clave subrogada es un identificador \u00fanico que es asignado a cada fila de la tabla de dimensiones, en definitiva, ser\u00e1 su clave principal. Clave de negocio es una clave que act\u00faa como primary key en nuestro origen de datos y es con la que el usuario est\u00e1 familiarizado, pero no puede ser clave principal en nuestra tabla de dimensiones porque se podr\u00edan producir duplicidades. T\u00e9cnica de modelado dimensional \u00b6 En el libro \u201cThe datawarehouse toolkit\u201d de Ralph Kimball, ofrece una t\u00e9cnica de modelado que consiste en 4 pasos: Seleccionar el proceso : El primer paso ser\u00eda seleccionar el proceso a modelar. \u00bfEn qu\u00e9 actividades del negocio nos interesa focalizar? Los modelos de negocio son simplificaciones de la realidad que nos sirven para comprender qu\u00e9 est\u00e1 sucediendo. Si est\u00e1 bien definido nos permitir\u00e1 responder preguntas claves de la organizaci\u00f3n. El o los procesos a modelar se definen de acuerdo al objetivo. Seguramente queramos modelar m\u00e1s de un proceso, cada uno con su tabla de hechos y dimensiones, y cada uno de ellos con sus propios objetivos e indicadores de gesti\u00f3n. Establecer el nivel de granularidad : Una vez identificado el proceso, tenemos que conocer qu\u00e9 informaci\u00f3n disponemos para analizarlo, definir el nivel de detalle que vamos a registrar en la tabla de hechos. Por ejemplo podemos definir las ventas en por mes, por producto, o bien las ventas por d\u00eda por vendedor. Hay que tener en cuenta que si se define un grano m\u00e1s grueso (por ejemplo, ventas por mes), no podr\u00e1 luego consultarse las ventas por d\u00eda. Por eso se recomienda utilizar una granularidad m\u00e1s fina, de manera de poder responder a consultas con mayor nivel de detalle. Identificar las dimensiones : Definida la granularidad, se enumeran las dimensiones asociadas. Las m\u00e1s comunes son, tiempos, lugar, responsable, sector, etc. Es muy importante nombrar las dimensiones de manera que sean simples de identificar, como tambi\u00e9n as\u00ed sus atributos. Identificar los hechos : Los hechos son los valores num\u00e9ricos asociados al proceso que queremos medir. Por ejemplo en un proceso de compras, ser\u00e1n la cantidad vendida y el valor unitario. En un proceso de atenci\u00f3n de reclamos podr\u00eda ser el tiempo en segundos que demor\u00f3 en ser atendido, el tiempo en segundos que tom\u00f3 la atenci\u00f3n, etc. Cubos OLAP \u00b6","title":"6. DATA WAREHOUSE"},{"location":"apuntes14.html#data-warehouse","text":"Es una gran base de datos, normalmente medida en gigabytes o terabytes. Es un proceso para recopilar y administrar datos de diversas fuentes para proporcionar informaci\u00f3n empresarial significativa (data warehousing). Conecta y analiza datos comerciales de fuentes heterog\u00e9neas. Es el n\u00facleo del sistema de Business Intelligence que est\u00e1 dise\u00f1ado para el an\u00e1lisis y generaci\u00f3n de informes de datos. Es un proceso de transformar datos en informaci\u00f3n y ponerlos a disposici\u00f3n de los usuarios. Contiene datos de calidad como datos de clientes, empleados o ventas.","title":"Data Warehouse"},{"location":"apuntes14.html#caracteristicas-de-los-data-warehouse","text":"","title":"Caracter\u00edsticas de los Data Warehouse"},{"location":"apuntes14.html#comparacion-modelo-transaccional-y-analitico","text":"","title":"Comparaci\u00f3n modelo transaccional y anal\u00edtico"},{"location":"apuntes14.html#data-mart","text":"Seg\u00fan Kimball, \u201cUn Data Mart es un conjunto de datos flexible, idealmente basado en el nivel de granularidad mayor que sea posible, presentado en un modelo dimensional que es capaz de comportarse bien ante cualquier consulta del usuario. En su definici\u00f3n m\u00e1s sencilla, un data mart representa un \u00fanico proceso de negocio.\u201d Un proceso de negocio es un conjunto de tareas relacionadas l\u00f3gicamente, llevadas a cabo para lograr un resultado de negocio definido.","title":"Data Mart"},{"location":"apuntes14.html#data-mart-vs-data-warehouse","text":"La \u00fanica diferencia es en cuanto al alcance. Un Data Warehouse es un sistema centralizado con datos globales de la empresa y de todos sus procesos operacionales mientras que un Data Mart es un subconjunto tem\u00e1tico de datos, orientado a un proceso o un \u00e1rea de negocio espec\u00edfica. Por ejemplo, a pedidos de clientes, a compras, a inventario de almac\u00e9n, etc. La existencia de un Data Warehouse no descarta la existencia de Data Marts ni viceversa. Es decir, puede haber organizaciones que tengan s\u00f3lo un Data Warehouse, que s\u00f3lo tengan Data Marts, o que tengan un Data Warehouse y Data Marts.","title":"Data Mart VS Data Warehouse"},{"location":"apuntes14.html#etl","text":"En muchas ocasiones, la informaci\u00f3n no pasa directamente de las fuentes al Data Mart o Data Warehouse, sino que lo hace a trav\u00e9s de unas bases de datos intermedias, que son necesarias dada la complejidad y disparidad de las fuentes. Los datos, antes de entrar en el Data Mart o Data Warehouse, se almacenan en un \u00e1rea de staging y/o un ODS (Operational Data Store). Un \u00e1rea de staging es un \u00e1rea temporal que se encuentra en el flujo de datos entre las fuentes y el Data Mart o Data Warehouse con el fin de facilitar la extracci\u00f3n de datos, de realizar tareas de limpieza (data cleasing) o de mejorar la calidad de los datos. Un ODS es un \u00e1rea que va a dar soporte a los sistemas transaccionales, desde los que se alimenta con una periodicidad muy baja, y sirve como base de datos de consulta, a la que se conectarn herramientas de reporting con el fin de que el sistema transaccional tenga una menor carga de trabajo.","title":"ETL"},{"location":"apuntes14.html#metodologias","text":"Existen muchas metodolog\u00edas de dise\u00f1o y construcci\u00f3n de un Data Warehouse. Cada fabricante de software de inteligencia de negocios busca imponer una metodolog\u00eda con sus productos. Sin embargo, se imponen entre la mayor\u00eda dos metodolog\u00edas, la de Kimball y la de Inmon. Metodolog\u00eda Kimball : Ralph Kimball sigue una metodolog\u00eda bottom-up y una estructura desnormalizada. Se define el dise\u00f1o en detalle de partes individuales para luego enlazarlas entre ellas para formar un sistema completo. Es decir, con la metodolog\u00eda de Kimball se van dise\u00f1ando a medida, seg\u00fan las necesidades de los departamentos, los diferentes Data Marts, para unirlos y formar el Data Warehouse de la empresa. Metodolog\u00eda Inmon :se caracteriza por tener un repositorio central de datos \u00fanico. Se basa en la metodolog\u00eda top-down, que es aquella que toma decisiones partiendo de las variables m\u00e1s globales para ir descendiendo hasta las m\u00e1s espec\u00edficas. Es decir, Inmon parte de un almac\u00e9n de datos global del que se ir\u00e1n nutriendo los diferentes Data Marts, evitando as\u00ed las diferentes inconsistencias que puedan existir entre diferentes departamentos. Alternativa: Data Vault : es una t\u00e9cnica h\u00edbrida entre el modelo normalizado y el modelo dimensional donde se toman lo mejor de cada una y se solucionan sus defectos. La definici\u00f3n que da Dan Listedt es la siguiente: \u201cData Vault es un conjunto de tablas normalizadas orientadas al detalle, con seguimiento hist\u00f3rico y vinculadas de forma \u00fanica, que dan soporte a una o m\u00e1s \u00e1reas funcionales del negocio. Se trata de un enfoque h\u00edbrido que engloba lo mejor de la tercera forma normal (3NF) y el esquema en estrella. El dise\u00f1o es flexible, escalable, consistente y adaptable a las necesidades de la empresa. Se trata de un modelo de datos dise\u00f1ado espec\u00edficamente para satisfacer las necesidades de los almacenes de datos empresariales actuales.\u201d Comparativa entre Inmon y Kimball:","title":"Metodolog\u00edas"},{"location":"apuntes14.html#modelado","text":"Esquema en estrella (star schema) : hay una \u00fanica tabla central, la tabla de hechos, que contiene todas las medidas y una tabla adicional por cada una de las perspectivas desde las que queremos analizar dicha informaci\u00f3n, es decir por cada una de las dimensiones. Esquema en copo de nieve (snowflake schema) . La diferencia es que algunas dimensiones no est\u00e1n relacionadas directamente con la tabla de hechos, sino que se relacionan con ella a trav\u00e9s de otras dimensiones. Tambi\u00e9n tenemos una tabla de hechos, situada en el centro, que contiene todas las medidas y una o varias tablas adicionales, con un mayor nivel de normalizaci\u00f3n. Esquema h\u00edbrido . Tiene parte en estrella y parte en copo de nieve.","title":"Modelado"},{"location":"apuntes14.html#modelado-dimensional","text":"Utilizado en la mayor\u00eda de las soluciones de BI. Es una mezcla de normalizaci\u00f3n y desnormalizaci\u00f3n, llamada Normalizaci\u00f3n Dimensional. Se utiliza tanto en el dise\u00f1o de Data Marts como de Data Warehouses. Hay dos tipos de tablas: Tablas de Dimensi\u00f3n (Dimension Tables) Tablas de Hechos (Fact Tables)","title":"Modelado Dimensional"},{"location":"apuntes14.html#tablas-de-hechos","text":"Son tablas que representan detalles del proceso de negocio a analizar, por ejemplo, las ventas, las compras, las incidencias recibidas, los pagos, los apuntes contables, los clics sobre nuestro sitio web, etc. Contienen datos num\u00e9ricos y medidas (m\u00e9tricas). Contienen tambi\u00e9n elementos (claves externas) para contextualizar dichas medidas, como por ejemplo el producto, la fecha, el cliente, la cuenta contable, etc. Elementos que las componen: Clave principal : identifica de forma \u00fanica cada fila. Claves externas : apuntan hacia las claves principales de cada una de las dimensiones que tienen relaci\u00f3n con dicha tabla de hechos. Medidas : representan columnas que contienen datos cuantificables, num\u00e9ricos, que se pueden agregar como por ejemplo, cantidad, importe, precio, margen, etc. Metadatos y linaje: permite obtener informaci\u00f3n adicional sobre la fila, por ejemplo, que d\u00eda se incorpor\u00f3 al Data Warehouse, de qu\u00e9 origen proviene, etc.","title":"Tablas de Hechos"},{"location":"apuntes14.html#tablas-de-dimensiones","text":"Las tablas de dimensiones almacenan una serie de atributos o caracter\u00edsticas, por las cuales podemos agrupar, rebanar o filtrar informaci\u00f3n. Nos permiten contextualizar los hechos, agregando diferentes perspectivas de an\u00e1lisis. Algunas veces los atributos est\u00e1n organizados en jerarqu\u00edas que permiten analizar los datos de forma agrupada. Por ejemplo, en una dimensi\u00f3n Producto podemos encontrar una jerarqu\u00eda formada por los atributos Categor\u00eda, Subcategor\u00eda y Producto. Tienen una clave principal diferente que se conoce con el nombre de clave subrogada. Clave subrogada es un identificador \u00fanico que es asignado a cada fila de la tabla de dimensiones, en definitiva, ser\u00e1 su clave principal. Clave de negocio es una clave que act\u00faa como primary key en nuestro origen de datos y es con la que el usuario est\u00e1 familiarizado, pero no puede ser clave principal en nuestra tabla de dimensiones porque se podr\u00edan producir duplicidades.","title":"Tablas de Dimensiones"},{"location":"apuntes14.html#tecnica-de-modelado-dimensional","text":"En el libro \u201cThe datawarehouse toolkit\u201d de Ralph Kimball, ofrece una t\u00e9cnica de modelado que consiste en 4 pasos: Seleccionar el proceso : El primer paso ser\u00eda seleccionar el proceso a modelar. \u00bfEn qu\u00e9 actividades del negocio nos interesa focalizar? Los modelos de negocio son simplificaciones de la realidad que nos sirven para comprender qu\u00e9 est\u00e1 sucediendo. Si est\u00e1 bien definido nos permitir\u00e1 responder preguntas claves de la organizaci\u00f3n. El o los procesos a modelar se definen de acuerdo al objetivo. Seguramente queramos modelar m\u00e1s de un proceso, cada uno con su tabla de hechos y dimensiones, y cada uno de ellos con sus propios objetivos e indicadores de gesti\u00f3n. Establecer el nivel de granularidad : Una vez identificado el proceso, tenemos que conocer qu\u00e9 informaci\u00f3n disponemos para analizarlo, definir el nivel de detalle que vamos a registrar en la tabla de hechos. Por ejemplo podemos definir las ventas en por mes, por producto, o bien las ventas por d\u00eda por vendedor. Hay que tener en cuenta que si se define un grano m\u00e1s grueso (por ejemplo, ventas por mes), no podr\u00e1 luego consultarse las ventas por d\u00eda. Por eso se recomienda utilizar una granularidad m\u00e1s fina, de manera de poder responder a consultas con mayor nivel de detalle. Identificar las dimensiones : Definida la granularidad, se enumeran las dimensiones asociadas. Las m\u00e1s comunes son, tiempos, lugar, responsable, sector, etc. Es muy importante nombrar las dimensiones de manera que sean simples de identificar, como tambi\u00e9n as\u00ed sus atributos. Identificar los hechos : Los hechos son los valores num\u00e9ricos asociados al proceso que queremos medir. Por ejemplo en un proceso de compras, ser\u00e1n la cantidad vendida y el valor unitario. En un proceso de atenci\u00f3n de reclamos podr\u00eda ser el tiempo en segundos que demor\u00f3 en ser atendido, el tiempo en segundos que tom\u00f3 la atenci\u00f3n, etc.","title":"T\u00e9cnica de modelado dimensional"},{"location":"apuntes14.html#cubos-olap","text":"","title":"Cubos OLAP"},{"location":"apuntes15.html","text":"Limpieza de datos \u00b6 La limpieza de datos (data cleaning o data scrubbing) consiste en el descubrimiento y correcci\u00f3n o eliminaci\u00f3n de datos err\u00f3neos en una tabla, base de datos o dataframe. El proceso de limpieza de datos permite identificar datos incompletos, incorrectos, inexactos, no pertinentes, etc y luego substituir, modificar o eliminar estos datos sucios (\"data duty\") . Los datos de una fuente, a la hora de consumirlos suelen ser: Inconsistentes : con informaci\u00f3n relevante y no relevante. Imprecisos : contienen datos incorrectos, valores perdidos, etc. Repetitivos : duplicidad de datos. Exploraci\u00f3n \u00b6 Una vez que tenemos descargado el dataset, lo primero que hay que hacer es reconocer con qu\u00e9 nos estamos enfrentando. Para ello visualizamos el dataset. df = pd . read_csv ( 'https://raw.githubusercontent.com/asalber/manual-python/master/datos/colesterol.csv' ) df . head () Tambi\u00e9n podemos visualizar desde abajo con tail. Para obtener datos r\u00e1pidos de lo que hay dentro, utilizamos el m\u00e9todo describe. df . describe () La funci\u00f3n describe nos muestra los datos relevantes para columnas que contengan n\u00fameros. Para obtener datos de columnas que contienen valores de otro tipo como por ejemplo cadenas, debemos agregar un par\u00e1metro para que tome en cuenta a todo el dataset y no solo a los valores num\u00e9ricos. df . describe ( include = 'all' ) Las tres categor\u00edas que se incluyen son: unique : cantidad de valores \u00fanicos. top : valor \u00fanico con m\u00e1s repeticiones. freq : cantidad de veces que aparece el valor m\u00e1s repetido. Eliminar las columnas irrelevantes \u00b6 Las columnas que no sean interesantes para el desarrollo de nuestro problema las podemos eliminar. En nuestro ejemplo vamos a eliminar la columna nombre pero como luego nos va a hacer falta para continuar con los ejemplos, vamos a asignar el dataframe a df1 y sobre \u00e9ste eliminamos el campo nombre. df1 = df df1 . drop ( 'nombre' , inplace = True , axis = 1 ) Tipos de datos \u00b6 Obtener los tipos de datos de las diferentes columnas para ver si son correctos. df . dtypes Si comprobamos que hay alg\u00fan tipo de dato err\u00f3neo podemos cambiarlo, por ejemplo el campo edad que es entero queremos cambiarlo a float: df [[ 'edad' ]] = df [[ 'edad' ]] . astype ( float ) df . dtypes Tratar con los datos perdidos \u00b6 Otro paso en la limpieza de datos es averiguar si el dataset est\u00e1 completo o tiene datos perdidos. missing_data = df . isnull () for column in missing_data . columns . values . tolist (): print ( column ) print ( missing_data [ column ] . value_counts ()) print ( '' ) Los valores perdidos los podemos reemplazar, para ello hay que averiguar qu\u00e9 tipo de dato es y c\u00f3mo tratarlo. Strings : la mejor pr\u00e1ctica es reemplazar el dato perdido por el que se repite con m\u00e1s frecuencia. Integers : el dato perdido debe reemplazarse por el promedio de su misma columna. En el caso de que sea strings : Primero comprobamos el valor que m\u00e1s se repite que vendr\u00e1 indicado en la etiqueta top . import numpy as np df . describe ( include = [ np . object ]) En el caso del ejemplo, como no hay ning\u00fan valor vac\u00edo en el campo nombre, vamos a eliminar el nombre de Rosa D\u00edaz D\u00edaz para luego poder reemplazarlo: df . replace ( 'Rosa D\u00edaz D\u00edaz' , np . nan , inplace = True ) Luego lo reemplazamos: df [ 'nombre' ] . replace ( np . nan , 'Jos\u00e9 Luis Martinez Izquierdo' , inplace = True En el caso de que sea integer : Se obtiene la media: media = df [ 'colesterol' ] . astype ( float ) . mean ( axis = 0 ) print ( media ) Reemplazamos el valor: df [ 'colesterol' ] . replace ( np . nan , media , inplace = True ) Reemplazar usando Media, Mediana o Moda \u00b6 df = pd . read_csv ( 'https://raw.githubusercontent.com/asalber/manual-python/master/datos/colesterol.csv' ) x = df [ \"colesterol\" ] . mean () df [ \"colesterol\" ] . fillna ( x , inplace = True ) Reemplazar el valor de las filas si es necesario \u00b6 Para aplicar mecanismos de inteligencia artificial, necesitamos que todos los datos sean num\u00e9ricos y nos podr\u00eda interesar modificar los valores de alg\u00fan campo a num\u00e9rico. En el caso del ejemplo, queremos cambiar el campo sexo y asignarle un 1 si es hombre y un 0 si es mujer. replace = { 'H' : 1 , 'M' : 0 } df = df . replace ({ \"sexo\" : replace }) Outliers \u00b6 Outlier es cualquier dato que difiere significativamente del promedio o rango observado del resto de nuestros datos. Estos valores se pueden detectar a simple vista revisando todos los datos de la columna o tambi\u00e9n mediante gr\u00e1ficos. Datos categ\u00f3ricos \u00b6 Los datos categ\u00f3ricos toman un n\u00famero limitado y generalmente fijo de valores posibles. Por ejemplo: g\u00e9nero, clase social, tipo de sangre, etc. Existen dos tipos de variables categ\u00f3ricas: Variables ordinales : sus valores pueden ser ordenados jer\u00e1rquicamente, como, por ejemplo, el nivel de educaci\u00f3n de una persona (sin estudios, primaria, secundaria o superiores). Variables nominales : no se puede establecer un orden en sus categor\u00edas. Algunos ejemplos son el sexo de una persona (hombre o mujer), el color del pelo (casta\u00f1o, rubio, negro o rojo) o el pa\u00eds de nacimiento (Espa\u00f1a, Italia, Francia, etc.). Realiza los siguientes ejercicios: Limpieza y preparaci\u00f3n I Limpieza y preparaci\u00f3n II","title":"Apuntes15"},{"location":"apuntes15.html#limpieza-de-datos","text":"La limpieza de datos (data cleaning o data scrubbing) consiste en el descubrimiento y correcci\u00f3n o eliminaci\u00f3n de datos err\u00f3neos en una tabla, base de datos o dataframe. El proceso de limpieza de datos permite identificar datos incompletos, incorrectos, inexactos, no pertinentes, etc y luego substituir, modificar o eliminar estos datos sucios (\"data duty\") . Los datos de una fuente, a la hora de consumirlos suelen ser: Inconsistentes : con informaci\u00f3n relevante y no relevante. Imprecisos : contienen datos incorrectos, valores perdidos, etc. Repetitivos : duplicidad de datos.","title":"Limpieza de datos"},{"location":"apuntes15.html#exploracion","text":"Una vez que tenemos descargado el dataset, lo primero que hay que hacer es reconocer con qu\u00e9 nos estamos enfrentando. Para ello visualizamos el dataset. df = pd . read_csv ( 'https://raw.githubusercontent.com/asalber/manual-python/master/datos/colesterol.csv' ) df . head () Tambi\u00e9n podemos visualizar desde abajo con tail. Para obtener datos r\u00e1pidos de lo que hay dentro, utilizamos el m\u00e9todo describe. df . describe () La funci\u00f3n describe nos muestra los datos relevantes para columnas que contengan n\u00fameros. Para obtener datos de columnas que contienen valores de otro tipo como por ejemplo cadenas, debemos agregar un par\u00e1metro para que tome en cuenta a todo el dataset y no solo a los valores num\u00e9ricos. df . describe ( include = 'all' ) Las tres categor\u00edas que se incluyen son: unique : cantidad de valores \u00fanicos. top : valor \u00fanico con m\u00e1s repeticiones. freq : cantidad de veces que aparece el valor m\u00e1s repetido.","title":"Exploraci\u00f3n"},{"location":"apuntes15.html#eliminar-las-columnas-irrelevantes","text":"Las columnas que no sean interesantes para el desarrollo de nuestro problema las podemos eliminar. En nuestro ejemplo vamos a eliminar la columna nombre pero como luego nos va a hacer falta para continuar con los ejemplos, vamos a asignar el dataframe a df1 y sobre \u00e9ste eliminamos el campo nombre. df1 = df df1 . drop ( 'nombre' , inplace = True , axis = 1 )","title":"Eliminar las columnas irrelevantes"},{"location":"apuntes15.html#tipos-de-datos","text":"Obtener los tipos de datos de las diferentes columnas para ver si son correctos. df . dtypes Si comprobamos que hay alg\u00fan tipo de dato err\u00f3neo podemos cambiarlo, por ejemplo el campo edad que es entero queremos cambiarlo a float: df [[ 'edad' ]] = df [[ 'edad' ]] . astype ( float ) df . dtypes","title":"Tipos de datos"},{"location":"apuntes15.html#tratar-con-los-datos-perdidos","text":"Otro paso en la limpieza de datos es averiguar si el dataset est\u00e1 completo o tiene datos perdidos. missing_data = df . isnull () for column in missing_data . columns . values . tolist (): print ( column ) print ( missing_data [ column ] . value_counts ()) print ( '' ) Los valores perdidos los podemos reemplazar, para ello hay que averiguar qu\u00e9 tipo de dato es y c\u00f3mo tratarlo. Strings : la mejor pr\u00e1ctica es reemplazar el dato perdido por el que se repite con m\u00e1s frecuencia. Integers : el dato perdido debe reemplazarse por el promedio de su misma columna. En el caso de que sea strings : Primero comprobamos el valor que m\u00e1s se repite que vendr\u00e1 indicado en la etiqueta top . import numpy as np df . describe ( include = [ np . object ]) En el caso del ejemplo, como no hay ning\u00fan valor vac\u00edo en el campo nombre, vamos a eliminar el nombre de Rosa D\u00edaz D\u00edaz para luego poder reemplazarlo: df . replace ( 'Rosa D\u00edaz D\u00edaz' , np . nan , inplace = True ) Luego lo reemplazamos: df [ 'nombre' ] . replace ( np . nan , 'Jos\u00e9 Luis Martinez Izquierdo' , inplace = True En el caso de que sea integer : Se obtiene la media: media = df [ 'colesterol' ] . astype ( float ) . mean ( axis = 0 ) print ( media ) Reemplazamos el valor: df [ 'colesterol' ] . replace ( np . nan , media , inplace = True )","title":"Tratar con los datos perdidos"},{"location":"apuntes15.html#reemplazar-usando-media-mediana-o-moda","text":"df = pd . read_csv ( 'https://raw.githubusercontent.com/asalber/manual-python/master/datos/colesterol.csv' ) x = df [ \"colesterol\" ] . mean () df [ \"colesterol\" ] . fillna ( x , inplace = True )","title":"Reemplazar usando Media, Mediana o Moda"},{"location":"apuntes15.html#reemplazar-el-valor-de-las-filas-si-es-necesario","text":"Para aplicar mecanismos de inteligencia artificial, necesitamos que todos los datos sean num\u00e9ricos y nos podr\u00eda interesar modificar los valores de alg\u00fan campo a num\u00e9rico. En el caso del ejemplo, queremos cambiar el campo sexo y asignarle un 1 si es hombre y un 0 si es mujer. replace = { 'H' : 1 , 'M' : 0 } df = df . replace ({ \"sexo\" : replace })","title":"Reemplazar el valor de las filas si es necesario"},{"location":"apuntes15.html#outliers","text":"Outlier es cualquier dato que difiere significativamente del promedio o rango observado del resto de nuestros datos. Estos valores se pueden detectar a simple vista revisando todos los datos de la columna o tambi\u00e9n mediante gr\u00e1ficos.","title":"Outliers"},{"location":"apuntes15.html#datos-categoricos","text":"Los datos categ\u00f3ricos toman un n\u00famero limitado y generalmente fijo de valores posibles. Por ejemplo: g\u00e9nero, clase social, tipo de sangre, etc. Existen dos tipos de variables categ\u00f3ricas: Variables ordinales : sus valores pueden ser ordenados jer\u00e1rquicamente, como, por ejemplo, el nivel de educaci\u00f3n de una persona (sin estudios, primaria, secundaria o superiores). Variables nominales : no se puede establecer un orden en sus categor\u00edas. Algunos ejemplos son el sexo de una persona (hombre o mujer), el color del pelo (casta\u00f1o, rubio, negro o rojo) o el pa\u00eds de nacimiento (Espa\u00f1a, Italia, Francia, etc.). Realiza los siguientes ejercicios: Limpieza y preparaci\u00f3n I Limpieza y preparaci\u00f3n II","title":"Datos categ\u00f3ricos"}]}